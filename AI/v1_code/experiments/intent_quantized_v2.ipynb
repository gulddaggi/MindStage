{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] FP32 모델 로드: /home/j-k13b204/S13P31B204/model_test/kobert_intent_v2/model_best.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for KoBertClassifier:\n\tUnexpected key(s) in state_dict: \"loss_fn.pos_weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 138\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INFO] 양자화된 모델 저장 완료: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUANTIZED_MODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    124\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# 1) FP32 모델 로드\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m fp32_model = \u001b[43mload_fp32_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# 2) PTQ 수행\u001b[39;00m\n\u001b[32m    130\u001b[39m q_model = quantize_model_ptq(fp32_model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mload_fp32_model\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m     84\u001b[39m     bert = get_kobert_model()  \u001b[38;5;66;03m# ★ 언패킹 X\u001b[39;00m\n\u001b[32m     85\u001b[39m     model = KoBertClassifier(\n\u001b[32m     86\u001b[39m         bert=bert,\n\u001b[32m     87\u001b[39m         hidden_size=\u001b[32m768\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m         pos_weight=\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# 추론 시엔 안 써도 됨\u001b[39;00m\n\u001b[32m     91\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# ==== case 2: torch.save(model, ...) 로 전체 모델 저장했을 때 ====\u001b[39;00m\n\u001b[32m     95\u001b[39m     model = obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for KoBertClassifier:\n\tUnexpected key(s) in state_dict: \"loss_fn.pos_weight\". "
     ]
    }
   ],
   "source": [
    "# quantize_kobert_intent_v2.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quant\n",
    "from kobert_transformers import get_kobert_model\n",
    "\n",
    "# ===== 0) 사용자 환경 =====\n",
    "MODEL_PATH = \"/home/j-k13b204/S13P31B204/model_test/kobert_intent_v2/model_best.pt\"\n",
    "SAVE_DIR = \"/home/j-k13b204/S13P31B204/model_test/kobert_intent_v2/quantize\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "QUANTIZED_PT_PATH = os.path.join(SAVE_DIR, \"model_best_quantized.pt\")\n",
    "\n",
    "# ===== 1) 모델 클래스 정의 (intent용, BCEWithLogits) =====\n",
    "class KoBertClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert,\n",
    "        hidden_size: int = 768,\n",
    "        num_classes: int = 5,\n",
    "        dr_rate: float = 0.3,\n",
    "        pos_weight: torch.Tensor | None = None,  # 각 클래스의 양성 가중치 (불균형 보정)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p=dr_rate) if dr_rate and dr_rate > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        if pos_weight is not None:\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        else:\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor | None = None,\n",
    "        token_type_ids: torch.Tensor | None = None,\n",
    "        labels: torch.Tensor | None = None,  # (B, num_classes), multi-hot\n",
    "    ):\n",
    "        out = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        if getattr(out, \"pooler_output\", None) is not None:\n",
    "            pooled = out.pooler_output      # (B, 768)\n",
    "        else:\n",
    "            pooled = out[0][:, 0]           # last_hidden_state[:, 0]\n",
    "\n",
    "        logits = self.classifier(self.dropout(pooled))  # (B, num_classes)\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            return probs, loss\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs, None\n",
    "\n",
    "# ===== 2) 모델 로드 함수 =====\n",
    "def load_fp32_model(model_path: str, num_classes: int = 5, dr_rate: float = 0.3) -> nn.Module:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"[1/3] 모델 구조 초기화…\")\n",
    "\n",
    "    bert = get_kobert_model()  # ★ 언패킹 없이 모델만 반환\n",
    "    model = KoBertClassifier(\n",
    "        bert=bert,\n",
    "        hidden_size=768,\n",
    "        num_classes=num_classes,\n",
    "        dr_rate=dr_rate,\n",
    "        pos_weight=None,   # 추론용이라 pos_weight 안 써도 됨\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"[2/3] 가중치 로드…\")\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # 학습 때 loss_fn.* 도 같이 저장돼서 걸리니까 제거\n",
    "    for k in list(state.keys()):\n",
    "        if k.startswith(\"loss_fn\"):\n",
    "            del state[k]\n",
    "\n",
    "    # 엄격하게 안 맞춰도 되니 strict=False\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    if missing:\n",
    "        print(f\"[WARN] missing keys: {missing}\")\n",
    "    if unexpected:\n",
    "        print(f\"[WARN] unexpected keys: {unexpected}\")\n",
    "\n",
    "    model.eval()\n",
    "    print(\"[3/3] 로드 완료.\")\n",
    "    return model\n",
    "\n",
    "# ===== 3) 양자화 함수 =====\n",
    "def quantize_dynamic_int8(model: nn.Module) -> nn.Module:\n",
    "    print(\"\\n=> 동적 양자화(INT8) 수행 중…\")\n",
    "    qmodel = quant.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8,\n",
    "    )\n",
    "    print(\"완료.\")\n",
    "    return qmodel\n",
    "\n",
    "# ===== 4) 실행부 =====\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_CLASSES = 5  # NCS 5대 역량\n",
    "\n",
    "    model = load_fp32_model(MODEL_PATH, num_classes=NUM_CLASSES, dr_rate=0.3)\n",
    "    qmodel = quantize_dynamic_int8(model)\n",
    "\n",
    "    # 저장\n",
    "    torch.save(qmodel, QUANTIZED_PT_PATH)\n",
    "    print(f\"\\n✅ 양자화된 intent 모델 저장 완료: {QUANTIZED_PT_PATH}\")\n",
    "\n",
    "    # 크기 비교\n",
    "    def size_mb(path): return os.path.getsize(path) / (1024**2)\n",
    "    orig = size_mb(MODEL_PATH)\n",
    "    quantized = size_mb(QUANTIZED_PT_PATH)\n",
    "    print(f\"\\n원본 모델 크기: {orig:.2f} MB\")\n",
    "    print(f\"양자화 모델 크기: {quantized:.2f} MB\")\n",
    "    print(f\"압축률: {orig/quantized:.2f}x, 크기 감소: {(orig - quantized)/orig * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca31fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
