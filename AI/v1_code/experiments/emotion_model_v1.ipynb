{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1415fc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>expression</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>멀티 채널은 기존에 우리가 사용하고 있는 채널을 말하는데 어 쉽게 이야기하면 티비나...</td>\n",
       "      <td>u-fact</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>반면에 옴니 채널은 온 오프라인 모바일 등 다양한 채널을 통해서 상품을 검색하고 구...</td>\n",
       "      <td>u-fact</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>예를 들어 스마트폰 근거리 통신 기술을 이용해서 마트 앞을 지나는 고객에게 쿠폰을 ...</td>\n",
       "      <td>u-fact</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>두 개에 차이점은 멀티 채널은 운영하고 있는 각 채널을 독립적으로 운영해서 온 오프...</td>\n",
       "      <td>u-fact</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>제가 중국에서 일 년 정도 유학 생활을 했었는데요.</td>\n",
       "      <td>u-fact</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text expression  sentiment\n",
       "0  멀티 채널은 기존에 우리가 사용하고 있는 채널을 말하는데 어 쉽게 이야기하면 티비나...     u-fact  uncertain\n",
       "1  반면에 옴니 채널은 온 오프라인 모바일 등 다양한 채널을 통해서 상품을 검색하고 구...     u-fact  uncertain\n",
       "2  예를 들어 스마트폰 근거리 통신 기술을 이용해서 마트 앞을 지나는 고객에게 쿠폰을 ...     u-fact  uncertain\n",
       "3  두 개에 차이점은 멀티 채널은 운영하고 있는 각 채널을 독립적으로 운영해서 온 오프...     u-fact  uncertain\n",
       "4                       제가 중국에서 일 년 정도 유학 생활을 했었는데요.     u-fact  uncertain"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "data = pd.read_csv(\"/home/j-k13b204/S13P31B204/model_test/Code/csv_collection/emotion_grouping.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a4774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_encoding\n",
       "0    9488\n",
       "2    8068\n",
       "1    2231\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\"uncertain\": 0, \"negative\": 1, \"positive\": 2}\n",
    "\n",
    "data[\"label_encoding\"] = (\n",
    "    data[\"sentiment\"]\n",
    "        .map(label_map)            # 문자열 → 숫자 매핑\n",
    "        .fillna(-1)                # 혹시 없는 값은 -1 처리 (unknown)\n",
    "        .astype(int)\n",
    ")\n",
    "\n",
    "data['label_encoding'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f557d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>expression</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label_encoding</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>어 죽음이 두려운 건 사실입니다.</td>\n",
       "      <td>n-anxiety</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>한동안 이런 부분 때문에 힘들어서 일 년 휴학도 했지만 계속 혼자 힘으로 이 대학 ...</td>\n",
       "      <td>n-distress</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>하지만 분명히 그 사람을 도울 이유도 없고 그렇다고 물론 나를 꼭 도와야 될 이유는...</td>\n",
       "      <td>n-sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>혼자 일을 하는 경우는 감정적으로 고독하다는 것 보다도 내가 모든 것을 결정하고 수...</td>\n",
       "      <td>n-anxiety</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>그때 틀린 것으로 인해서 대학교의 레벨이 갈릴 수 있는 그런 큰 문제였기 때문에 그...</td>\n",
       "      <td>n-distress</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19687</th>\n",
       "      <td>마음이 너무 약해 남이 아파할 것을 먼저 생각하고 남의 마음을 먼저 생각해다 보니 ...</td>\n",
       "      <td>n-distress</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19691</th>\n",
       "      <td>또 제 나이에서 오는 어떤 신체적인   그런 위험 어떤 불안함도 있을 수 있구요.</td>\n",
       "      <td>n-anxiety</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19712</th>\n",
       "      <td>이 문제를 다른 사람에게 말하는 것부터 어린 시절 소심한 저에게 있어서 부끄럽고 어...</td>\n",
       "      <td>n-shame</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19718</th>\n",
       "      <td>그냥 저는 몰려오는 우울감에 항상 어려움을 겪었습니다.</td>\n",
       "      <td>n-distress</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19722</th>\n",
       "      <td>물론 협업하는 과제가   정말 협업이 잘되고 잘 맞는다면 정말 그것만큼 재미있는 과...</td>\n",
       "      <td>n-distress</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2231 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  expression  \\\n",
       "31                                    어 죽음이 두려운 건 사실입니다.   n-anxiety   \n",
       "45     한동안 이런 부분 때문에 힘들어서 일 년 휴학도 했지만 계속 혼자 힘으로 이 대학 ...  n-distress   \n",
       "85     하지만 분명히 그 사람을 도울 이유도 없고 그렇다고 물론 나를 꼭 도와야 될 이유는...   n-sadness   \n",
       "90     혼자 일을 하는 경우는 감정적으로 고독하다는 것 보다도 내가 모든 것을 결정하고 수...   n-anxiety   \n",
       "108    그때 틀린 것으로 인해서 대학교의 레벨이 갈릴 수 있는 그런 큰 문제였기 때문에 그...  n-distress   \n",
       "...                                                  ...         ...   \n",
       "19687  마음이 너무 약해 남이 아파할 것을 먼저 생각하고 남의 마음을 먼저 생각해다 보니 ...  n-distress   \n",
       "19691      또 제 나이에서 오는 어떤 신체적인   그런 위험 어떤 불안함도 있을 수 있구요.   n-anxiety   \n",
       "19712  이 문제를 다른 사람에게 말하는 것부터 어린 시절 소심한 저에게 있어서 부끄럽고 어...     n-shame   \n",
       "19718                     그냥 저는 몰려오는 우울감에 항상 어려움을 겪었습니다.  n-distress   \n",
       "19722  물론 협업하는 과제가   정말 협업이 잘되고 잘 맞는다면 정말 그것만큼 재미있는 과...  n-distress   \n",
       "\n",
       "      sentiment  label_encoding  len  \n",
       "31     negative               1   18  \n",
       "45     negative               1   63  \n",
       "85     negative               1  134  \n",
       "90     negative               1   89  \n",
       "108    negative               1   66  \n",
       "...         ...             ...  ...  \n",
       "19687  negative               1  104  \n",
       "19691  negative               1   45  \n",
       "19712  negative               1   56  \n",
       "19718  negative               1   30  \n",
       "19722  negative               1  116  \n",
       "\n",
       "[2231 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['label_encoding'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c07d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.53297619649264\n",
      "506\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "data['len'] = data['text'].str.len()\n",
    "print(data['len'].mean())\n",
    "print(data['len'].max())\n",
    "print(data['len'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "540679fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7534"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = (data['len'] >= 75).sum()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689afe5d",
   "metadata": {},
   "source": [
    "## 모델 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714a7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW    \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup \n",
    "\n",
    "from kobert_transformers import get_kobert_model, get_tokenizer\n",
    "\n",
    "# BERT 모델/토크나이저\n",
    "bertmodel = get_kobert_model()\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91074121",
   "metadata": {},
   "source": [
    "#### SoftMax + CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "151fffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "NUM_LABELS   = 3   # 0 : uncertain, 1 : negative, 2 : positive\n",
    "MAX_LEN      = 256\n",
    "BATCH_SIZE   = 16\n",
    "EPOCHS       = 10\n",
    "LR           = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "DROPOUT      = 0.2\n",
    "PATIENCE     = 2  # validation이 2epoch동안 증가 안 할시 멈춰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "# MAX_LEN 안전선\n",
    "assert MAX_LEN <= 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class KoBERTCLSClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert,\n",
    "        hidden_size: int = 768,\n",
    "        num_classes: int = 3,           # 0: uncertain, 1: negative, 2: positive\n",
    "        dropout: float = 0.3,           \n",
    "        class_weights: torch.Tensor | None = None,\n",
    "        label_smoothing: float = 0.0,   # 원하면 활성화 가능\n",
    "        freeze_bert: bool = False       # 초반 동결 여부\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # 감정 분류용 → CrossEntropyLoss\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=class_weights,\n",
    "            label_smoothing=label_smoothing\n",
    "        ) if class_weights is not None else nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor | None = None,\n",
    "        token_type_ids: torch.Tensor | None = None,\n",
    "        labels: torch.Tensor | None = None,\n",
    "    ):\n",
    "        # 1️⃣ KoBERT 인코더 통과\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        # 2️⃣ [CLS] 토큰 벡터 사용\n",
    "        if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            pooled = outputs.pooler_output             # (B, 768)\n",
    "        else:\n",
    "            pooled = outputs[0][:, 0]                  # last_hidden_state[:, 0]\n",
    "\n",
    "        # 3️⃣ Dropout → Linear\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)               # (B, num_classes)\n",
    "\n",
    "        # 4️⃣ Loss 계산 (학습 시)\n",
    "        if labels is not None:\n",
    "            if labels.dtype != torch.long:\n",
    "                labels = labels.long()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return logits, loss\n",
    "\n",
    "        return logits, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c3afcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'expression', 'sentiment', 'label_encoding', 'len'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee24aac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_encoding\n",
       "0    9488\n",
       "2    8068\n",
       "1    2231\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label_encoding'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc833a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weights: tensor([0.6952, 2.9556, 0.8175], device='cuda:0')\n",
      "[6641 1562 5647]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    data.dropna(subset =['text','label_encoding']),\n",
    "    test_size = 0.3, random_state = 42, stratify=data['label_encoding']\n",
    ")\n",
    "counts = train_df['label_encoding'].value_counts().sort_index().to_numpy()\n",
    "N, K = counts.sum(), counts.shape[0]\n",
    "class_weights = torch.tensor([float(N/(counts[i]*K)) for i in range(K)], dtype=torch.float32).to(device)\n",
    "print(\"class_weights:\", class_weights)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5930fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 한 문장씩 KoBert토크나이즈\n",
    "class IntentDataset(Dataset) : \n",
    "    def __init__(self, data, tokenizer, max_len) : \n",
    "        self.texts = data['text'].astype(str).tolist()\n",
    "        self.labels = data['label_encoding'].astype(int).tolist()\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) : return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        enc = self.tok(\n",
    "            self.texts[idx],\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_len,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\" : enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\" : enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\" : torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        if \"token_type_ids\" in enc : \n",
    "            item[\"token_type_ids\"] = enc[\"token_type_ids\"].squeeze(0)\n",
    "        else : \n",
    "            item['token_type_ids'] = None\n",
    "        return item\n",
    "\n",
    "# 미니배치 단위로 텐서 묶음\n",
    "train_loader = DataLoader(IntentDataset(train_df, tokenizer, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(IntentDataset(valid_df, tokenizer, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6447b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 옵티마이저, 스케쥴러 세팅\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "model = KoBERTCLSClassifier(\n",
    "    bert = bertmodel,\n",
    "    hidden_size=768,\n",
    "    num_classes = NUM_LABELS,\n",
    "    dropout=DROPOUT,\n",
    "    class_weights=class_weights\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * 0.06)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== EPOCH 1/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████| 866/866 [02:09<00:00,  6.71it/s]\n",
      "Valid: 100%|█████████████████| 372/372 [00:14<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.3717 acc=0.9581 f1=0.9566\n",
      "[Valid] loss=0.5211 acc=0.9469 f1=0.9455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   uncertain     0.9429    0.9505    0.9467      2847\n",
      "    negative     0.9654    0.9178    0.9410       669\n",
      "    positive     0.9469    0.9508    0.9489      2421\n",
      "\n",
      "    accuracy                         0.9469      5937\n",
      "   macro avg     0.9517    0.9397    0.9455      5937\n",
      "weighted avg     0.9471    0.9469    0.9469      5937\n",
      "\n",
      ">>> Best saved. macroF1=0.9455\n",
      "\n",
      "==== EPOCH 2/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████| 866/866 [02:06<00:00,  6.86it/s]\n",
      "Valid: 100%|█████████████████| 372/372 [00:14<00:00, 25.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.3109 acc=0.9674 f1=0.9666\n",
      "[Valid] loss=0.5118 acc=0.9446 f1=0.9442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   uncertain     0.9661    0.9203    0.9426      2847\n",
      "    negative     0.9264    0.9596    0.9427       669\n",
      "    positive     0.9265    0.9690    0.9473      2421\n",
      "\n",
      "    accuracy                         0.9446      5937\n",
      "   macro avg     0.9397    0.9496    0.9442      5937\n",
      "weighted avg     0.9455    0.9446    0.9445      5937\n",
      "\n",
      "\n",
      "==== EPOCH 3/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████| 866/866 [02:06<00:00,  6.84it/s]\n",
      "Valid: 100%|█████████████████| 372/372 [00:14<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.2019 acc=0.9792 f1=0.9781\n",
      "[Valid] loss=0.6109 acc=0.9488 f1=0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   uncertain     0.9495    0.9452    0.9474      2847\n",
      "    negative     0.9544    0.9387    0.9465       669\n",
      "    positive     0.9464    0.9558    0.9511      2421\n",
      "\n",
      "    accuracy                         0.9488      5937\n",
      "   macro avg     0.9501    0.9466    0.9483      5937\n",
      "weighted avg     0.9488    0.9488    0.9488      5937\n",
      "\n",
      ">>> Best saved. macroF1=0.9483\n",
      "\n",
      "==== EPOCH 4/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████| 866/866 [01:31<00:00,  9.48it/s]\n",
      "Valid: 100%|█████████████████| 372/372 [00:07<00:00, 47.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.1298 acc=0.9873 f1=0.9874\n",
      "[Valid] loss=0.5841 acc=0.9506 f1=0.9501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   uncertain     0.9559    0.9431    0.9494      2847\n",
      "    negative     0.9437    0.9522    0.9479       669\n",
      "    positive     0.9466    0.9591    0.9528      2421\n",
      "\n",
      "    accuracy                         0.9506      5937\n",
      "   macro avg     0.9487    0.9515    0.9501      5937\n",
      "weighted avg     0.9507    0.9506    0.9506      5937\n",
      "\n",
      ">>> Best saved. macroF1=0.9501\n",
      "\n",
      "==== EPOCH 5/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████| 866/866 [01:31<00:00,  9.50it/s]\n",
      "Valid: 100%|█████████████████| 372/372 [00:14<00:00, 26.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.0860 acc=0.9916 f1=0.9914\n",
      "[Valid] loss=0.6343 acc=0.9510 f1=0.9490\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   uncertain     0.9489    0.9526    0.9507      2847\n",
      "    negative     0.9392    0.9462    0.9427       669\n",
      "    positive     0.9568    0.9504    0.9536      2421\n",
      "\n",
      "    accuracy                         0.9510      5937\n",
      "   macro avg     0.9483    0.9497    0.9490      5937\n",
      "weighted avg     0.9510    0.9510    0.9510      5937\n",
      "\n",
      "\n",
      "==== EPOCH 6/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████| 866/866 [02:06<00:00,  6.85it/s]\n",
      "Valid: 100%|█████████████████| 372/372 [00:14<00:00, 25.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.0616 acc=0.9936 f1=0.9936\n",
      "[Valid] loss=0.5865 acc=0.9491 f1=0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   uncertain     0.9617    0.9350    0.9482      2847\n",
      "    negative     0.9201    0.9641    0.9416       669\n",
      "    positive     0.9433    0.9616    0.9523      2421\n",
      "\n",
      "    accuracy                         0.9491      5937\n",
      "   macro avg     0.9417    0.9536    0.9474      5937\n",
      "weighted avg     0.9495    0.9491    0.9491      5937\n",
      "\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === 학습/검증 루프 (감정 3클래스) ===\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "LABEL2ID = {'uncertain': 0, 'negative': 1, 'positive': 2}\n",
    "ID2LABEL = {v:k for k,v in LABEL2ID.items()}\n",
    "label_names = [ID2LABEL[i] for i in range(3)]  # ['uncertain','negative','positive']\n",
    "\n",
    "def _to_device(batch, device):\n",
    "    input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "    token_type_ids = batch.get(\"token_type_ids\", None)\n",
    "    if token_type_ids is None:\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "    token_type_ids = token_type_ids.to(device, non_blocking=True)\n",
    "    labels         = batch[\"labels\"].to(device, non_blocking=True)\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, all_p, all_y = 0.0, [], []\n",
    "    for batch in tqdm(loader, desc=\"Train\"):\n",
    "        input_ids, attention_mask, token_type_ids, labels = _to_device(batch, device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            logits, loss = model(input_ids, attention_mask, token_type_ids, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_p.extend(preds.detach().cpu().tolist())\n",
    "        all_y.extend(labels.detach().cpu().tolist())\n",
    "\n",
    "    denom = max(1, len(loader))\n",
    "    return total_loss/denom, accuracy_score(all_y, all_p), f1_score(all_y, all_p, average=\"macro\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, target_names=None):\n",
    "    model.eval()\n",
    "    total_loss, all_p, all_y = 0.0, [], []\n",
    "    for batch in tqdm(loader, desc=\"Valid\"):\n",
    "        input_ids, attention_mask, token_type_ids, labels = _to_device(batch, device)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            logits, loss = model(input_ids, attention_mask, token_type_ids, labels)\n",
    "        if loss is not None:\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_p.extend(preds.detach().cpu().tolist())\n",
    "        all_y.extend(labels.detach().cpu().tolist())\n",
    "\n",
    "    denom = max(1, len(loader))\n",
    "    rep = classification_report(\n",
    "        all_y, all_p,\n",
    "        labels=[0,1,2],                  # 순서 고정\n",
    "        target_names=target_names,       # ['uncertain','negative','positive']\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "    return total_loss/denom, accuracy_score(all_y, all_p), f1_score(all_y, all_p, average=\"macro\"), rep\n",
    "\n",
    "# === 학습 실행 ===\n",
    "best_f1, patience = 0.0, 0\n",
    "save_dir = \"./kobert_sentiment_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n==== EPOCH {epoch}/{EPOCHS} ====\")\n",
    "    tr_loss, tr_acc, tr_f1 = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc, val_f1, val_rep = evaluate(model, valid_loader, target_names=label_names)\n",
    "\n",
    "    print(f\"[Train] loss={tr_loss:.4f} acc={tr_acc:.4f} f1={tr_f1:.4f}\")\n",
    "    print(f\"[Valid] loss={val_loss:.4f} acc={val_acc:.4f} f1={val_f1:.4f}\")\n",
    "    print(val_rep)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1, patience = val_f1, 0\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"model.pt\"))\n",
    "        with open(os.path.join(save_dir, \"label_map.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, name in enumerate(label_names):   # 0→uncertain, 1→negative, 2→positive\n",
    "                f.write(f\"{i}\\t{name}\\n\")\n",
    "        print(f\">>> Best saved. macroF1={best_f1:.4f}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a7f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sentence-level ===\n",
      "- 저는 데이터를 통해 더 나은 판단을 만들 수 있다고 믿습니다.\n",
      "  -> pred: uncertain  probs={'uncertain': 0.9999991655349731, 'negative': 1.0574576236876965e-07, 'positive': 7.207909789030964e-07}\n",
      "- SSAFY 교육 과정에서 Java, Python을 기반으로 웹·데이터 분석 프로젝트를 수행하며 문제를 구조적으로 바라보는 습관을 기르게 되었습니다.\n",
      "  -> pred: uncertain  probs={'uncertain': 0.9999980926513672, 'negative': 9.087739272217732e-08, 'positive': 1.75665013557591e-06}\n",
      "- 특히 “성남시 공영주차장 입지 분석” 프로젝트에서는 실제 행정 데이터를 활용해 선형회귀 모델을 구축하고, 상권 밀집도·불법주정차 건수 등 변수를 조합해 정책 관점의 시나리오 분석을 진행했습니다.\n",
      "  -> pred: uncertain  probs={'uncertain': 0.999998927116394, 'negative': 1.1433208868538713e-07, 'positive': 1.0087101145472843e-06}\n",
      "- 이 경험을 통해 단순히 모델을 만드는 것을 넘어, 데이터를 기반으로 실제 이해관계자가 의사결정을 내릴 수 있도록 해석하는 능력을 갖추게 되었습니다.\n",
      "  -> pred: uncertain  probs={'uncertain': 0.9999983310699463, 'negative': 9.653924593067131e-08, 'positive': 1.5832737290111254e-06}\n",
      "- 앞으로도 데이터를 통해 문제를 명확하게 정의하고, 구조화된 방식으로 해결하는 데이터 분석가가 되고 싶습니다.\n",
      "  -> pred: positive  probs={'uncertain': 3.0188539312803186e-07, 'negative': 1.6435998873021163e-07, 'positive': 0.9999995231628418}\n",
      "\n",
      "=== Paragraph-level (mean of probs) ===\n",
      "{'paragraph_pred_id': 0, 'paragraph_pred_label': 'uncertain', 'paragraph_probs': {'uncertain': 0.799998939037323, 'negative': 1.14370891424187e-07, 'positive': 0.20000092685222626}}\n"
     ]
    }
   ],
   "source": [
    "# ==== Inference: Load saved model and predict per sentence ====\n",
    "import re, torch\n",
    "import torch.nn as nn\n",
    "from kobert_transformers import get_kobert_model, get_tokenizer\n",
    "\n",
    "# ---- 1) 하이퍼파라미터 & 라벨 매핑 ----\n",
    "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 256\n",
    "LABEL2ID = {'uncertain': 0, 'negative': 1, 'positive': 2}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "# ---- 2) 모델 클래스 (학습 때 쓴 것과 동일) ----\n",
    "class KoBERTCLSClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=3, dropout=0.3,\n",
    "                 class_weights=None, label_smoothing=0.0, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        # 추론에서는 loss를 쓰지 않지만, 키 불일치 방지를 위해 속성은 유지\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=class_weights,\n",
    "            label_smoothing=label_smoothing\n",
    "        ) if class_weights is not None else nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None \\\n",
    "                 else outputs[0][:, 0]  # [CLS]\n",
    "        logits = self.classifier(self.dropout(pooled))\n",
    "        if labels is not None:\n",
    "            if labels.dtype != torch.long:\n",
    "                labels = labels.long()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "\n",
    "# ---- 3) 모델/토크나이저 로드 & 가중치 불러오기 ----\n",
    "bert = get_kobert_model()\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "model = KoBERTCLSClassifier(\n",
    "    bert=bert,\n",
    "    hidden_size=768,\n",
    "    num_classes=3,\n",
    "    dropout=0.2  # 학습 시 사용값과 맞추면 가장 좋음\n",
    ").to(DEVICE)\n",
    "\n",
    "ckpt_path = \"/home/j-k13b204/S13P31B204/model_test/kobert_sentiment_model/model.pt\"\n",
    "state = torch.load(ckpt_path, map_location=DEVICE)\n",
    "_ = model.load_state_dict(state, strict=False)  # <- 핵심: loss_fn.weight 등 키 불일치 무시\n",
    "model.eval()\n",
    "\n",
    "# ---- 4) 문장 단위 분할 함수 ----\n",
    "def split_sentences(paragraph: str):\n",
    "    # 줄 단위 → 종결부호(., ?, !) 기준 추가 분할, 한글 종결('다.', '요.') 보강\n",
    "    lines = [s.strip() for s in paragraph.strip().splitlines() if s.strip()]\n",
    "    sents = []\n",
    "    for line in lines:\n",
    "        pieces = re.split(r'(?<=[\\.!?])\\s+|(?<=다\\.)\\s+|(?<=요\\.)\\s+', line)\n",
    "        sents += [p.strip() for p in pieces if p and p.strip()]\n",
    "    return sents\n",
    "\n",
    "# ---- 5) 예측 함수 ----\n",
    "@torch.no_grad()\n",
    "def predict_sentences(sent_list):\n",
    "    results, probs_all = [], []\n",
    "    for sent in sent_list:\n",
    "        enc = tokenizer(\n",
    "            sent,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        input_ids      = enc[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "        token_type_ids = enc.get(\"token_type_ids\", torch.zeros_like(attention_mask)).to(DEVICE)\n",
    "\n",
    "        logits, _ = model(input_ids, attention_mask, token_type_ids, labels=None)\n",
    "        prob = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()  # [3]\n",
    "        pred_id = int(prob.argmax())\n",
    "        results.append({\n",
    "            \"text\": sent,\n",
    "            \"pred_id\": pred_id,\n",
    "            \"pred_label\": ID2LABEL[pred_id],\n",
    "            \"probs\": {\n",
    "                \"uncertain\": float(prob[LABEL2ID['uncertain']]),\n",
    "                \"negative\":  float(prob[LABEL2ID['negative']]),\n",
    "                \"positive\":  float(prob[LABEL2ID['positive']]),\n",
    "            }\n",
    "        })\n",
    "        probs_all.append(prob)\n",
    "\n",
    "    # 문단 전체 스코어(평균 확률 기반)\n",
    "    whole_pred = None\n",
    "    if probs_all:\n",
    "        import numpy as np\n",
    "        mean_prob = np.stack(probs_all, axis=0).mean(axis=0)\n",
    "        whole_pred = {\n",
    "            \"paragraph_pred_id\": int(mean_prob.argmax()),\n",
    "            \"paragraph_pred_label\": ID2LABEL[int(mean_prob.argmax())],\n",
    "            \"paragraph_probs\": {\n",
    "                \"uncertain\": float(mean_prob[LABEL2ID['uncertain']]),\n",
    "                \"negative\":  float(mean_prob[LABEL2ID['negative']]),\n",
    "                \"positive\":  float(mean_prob[LABEL2ID['positive']]),\n",
    "            }\n",
    "        }\n",
    "    return results, whole_pred\n",
    "\n",
    "# ---- 6) 예측 실행 ----\n",
    "paragraph = \"\"\"\n",
    "저는 데이터를 통해 더 나은 판단을 만들 수 있다고 믿습니다. SSAFY 교육 과정에서 Java, Python을 기반으로 웹·데이터 분석 프로젝트를 수행하며 문제를 구조적으로 바라보는 습관을 기르게 되었습니다.\n",
    "특히 “성남시 공영주차장 입지 분석” 프로젝트에서는 실제 행정 데이터를 활용해 선형회귀 모델을 구축하고, 상권 밀집도·불법주정차 건수 등 변수를 조합해 정책 관점의 시나리오 분석을 진행했습니다.\n",
    "\n",
    "이 경험을 통해 단순히 모델을 만드는 것을 넘어, 데이터를 기반으로 실제 이해관계자가 의사결정을 내릴 수 있도록 해석하는 능력을 갖추게 되었습니다. 앞으로도 데이터를 통해 문제를 명확하게 정의하고, 구조화된 방식으로 해결하는 데이터 분석가가 되고 싶습니다.\n",
    "\"\"\"\n",
    "\n",
    "sents = split_sentences(paragraph)\n",
    "sent_results, para_result = predict_sentences(sents)\n",
    "\n",
    "print(\"=== Sentence-level ===\")\n",
    "for r in sent_results:\n",
    "    print(f\"- {r['text']}\")\n",
    "    print(f\"  -> pred: {r['pred_label']}  probs={r['probs']}\")\n",
    "\n",
    "print(\"\\n=== Paragraph-level (mean of probs) ===\")\n",
    "print(para_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize_kobert.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quant\n",
    "from kobert_transformers import get_kobert_model\n",
    "\n",
    "# ===== 0) 사용자 환경 =====\n",
    "MODEL_PATH = \"/home/j-k13b204/S13P31B204/model_test/kobert_sentiment_model/model.pt\"  # 네가 준 경로\n",
    "OUTPUT_DIR = os.path.join(os.path.dirname(MODEL_PATH), \"quantize\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "QUANTIZED_PT_PATH = os.path.join(OUTPUT_DIR, \"model_quantized.pt\")\n",
    "\n",
    "# ===== 1) 분류기 정의 (학습 때 쓰던 구조와 동일해야 함) =====\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=3, dr_rate=0.3, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p=dr_rate) if dr_rate and dr_rate > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        # 학습용 손실함수 키가 state_dict에 섞여있을 수 있어 정의만 남김\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out[0][:, 0]\n",
    "        logits = self.classifier(self.dropout(pooled))\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "\n",
    "def load_fp32_model(model_path: str, num_classes: int = 3, dr_rate: float = 0.3) -> nn.Module:\n",
    "    device = torch.device(\"cpu\")  # 동적 양자화는 CPU 대상\n",
    "    print(\"[1/3] 모델 구조 초기화…\")\n",
    "    bert = get_kobert_model()\n",
    "    model = BertClassifier(bert=bert, hidden_size=768, num_classes=num_classes, dr_rate=dr_rate).to(device)\n",
    "\n",
    "    print(\"[2/3] 가중치 로드…\")\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # state_dict에 학습용 키(예: 'loss_fn.weight')가 섞여 있을 수 있으니 제거\n",
    "    for k in list(state.keys()):\n",
    "        if k.startswith(\"loss_fn\"):\n",
    "            del state[k]\n",
    "\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model.eval()\n",
    "    print(\"[3/3] 로드 완료.\")\n",
    "    return model\n",
    "\n",
    "def quantize_dynamic_int8(model: nn.Module) -> nn.Module:\n",
    "    print(\"\\n=> 동적 양자화(INT8) 진행 중… (nn.Linear 대상)\")\n",
    "    qmodel = quant.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},        # Linear 층만 INT8\n",
    "        dtype=torch.qint8   # 가중치 정수화\n",
    "    )\n",
    "    print(\"   완료.\")\n",
    "    return qmodel\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 필요 시 여기만 바꿔: 감정 모델이면 num_classes=3(예: 부정/중립/긍정)\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    # 1) FP32 모델 로드\n",
    "    model = load_fp32_model(MODEL_PATH, num_classes=NUM_CLASSES, dr_rate=0.3)\n",
    "\n",
    "    # 2) INT8 동적 양자화\n",
    "    qmodel = quantize_dynamic_int8(model)\n",
    "\n",
    "    # 3) 저장 (모듈 통째 저장이 재로딩에 가장 안전)\n",
    "    torch.save(qmodel, QUANTIZED_PT_PATH)\n",
    "\n",
    "    # 4) 사이즈 출력\n",
    "    def size_mb(p): return os.path.getsize(p) / (1024 ** 2)\n",
    "    orig_mb = size_mb(MODEL_PATH)\n",
    "    quant_mb = size_mb(QUANTIZED_PT_PATH)\n",
    "    print(f\"\\n원본 모델:      {orig_mb:.2f} MB\")\n",
    "    print(f\"양자화 모델(.pt): {quant_mb:.2f} MB\")\n",
    "    if orig_mb > 0:\n",
    "        print(f\"압축률: {orig_mb/quant_mb:.2f}x, 크기 감소: {(orig_mb-quant_mb)/orig_mb*100:.1f}%\")\n",
    "\n",
    "    print(\"\\n✅ 완료!  (CPU 추론에서 메모리/지연시간 절감 효과 기대)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c38df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] 모델 구조 초기화…\n",
      "[2/3] 가중치 로드…\n",
      "[3/3] 로드 완료.\n",
      "\n",
      "=> 동적 양자화(INT8) 수행 중…\n",
      "   완료.\n",
      "\n",
      "✅ 양자화된 모델 저장 완료: /home/j-k13b204/S13P31B204/model_test/kobert_sentiment_model/quantize/model_quantized.pt\n",
      "\n",
      "원본 모델 크기: 351.74 MB\n",
      "양자화 모델 크기: 107.13 MB\n",
      "압축률: 3.28x, 크기 감소: 69.5%\n"
     ]
    }
   ],
   "source": [
    "# quantize_kobert_sentiment.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quant\n",
    "from kobert_transformers import get_kobert_model\n",
    "\n",
    "# ===== 0) 사용자 환경 =====\n",
    "MODEL_PATH = \"/home/j-k13b204/S13P31B204/model_test/kobert_sentiment_model/model.pt\"\n",
    "SAVE_DIR = \"/home/j-k13b204/S13P31B204/model_test/kobert_sentiment_model/quantize\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "QUANTIZED_PT_PATH = os.path.join(SAVE_DIR, \"model_quantized.pt\")\n",
    "\n",
    "# ===== 1) 모델 클래스 정의 =====\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=3, dr_rate=0.3, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p=dr_rate) if dr_rate and dr_rate > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out[0][:, 0]\n",
    "        logits = self.classifier(self.dropout(pooled))\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "\n",
    "# ===== 2) 모델 로드 함수 =====\n",
    "def load_fp32_model(model_path: str, num_classes: int = 3, dr_rate: float = 0.3) -> nn.Module:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"[1/3] 모델 구조 초기화…\")\n",
    "    bert = get_kobert_model()\n",
    "    model = BertClassifier(bert=bert, hidden_size=768, num_classes=num_classes, dr_rate=dr_rate).to(device)\n",
    "\n",
    "    print(\"[2/3] 가중치 로드…\")\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    for k in list(state.keys()):\n",
    "        if k.startswith(\"loss_fn\"):\n",
    "            del state[k]\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model.eval()\n",
    "    print(\"[3/3] 로드 완료.\")\n",
    "    return model\n",
    "\n",
    "# ===== 3) 양자화 함수 =====\n",
    "def quantize_dynamic_int8(model: nn.Module) -> nn.Module:\n",
    "    print(\"\\n=> 동적 양자화(INT8) 수행 중…\")\n",
    "    qmodel = quant.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    print(\"완료.\")\n",
    "    return qmodel\n",
    "\n",
    "# ===== 4) 실행부 =====\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_CLASSES = 3  # 감정 3분류 (부정/중립/긍정)\n",
    "\n",
    "    model = load_fp32_model(MODEL_PATH, num_classes=NUM_CLASSES, dr_rate=0.3)\n",
    "    qmodel = quantize_dynamic_int8(model)\n",
    "\n",
    "    # 저장\n",
    "    torch.save(qmodel, QUANTIZED_PT_PATH)\n",
    "    print(f\"\\n✅ 양자화된 모델 저장 완료: {QUANTIZED_PT_PATH}\")\n",
    "\n",
    "    # 크기 비교\n",
    "    def size_mb(path): return os.path.getsize(path) / (1024**2)\n",
    "    orig = size_mb(MODEL_PATH)\n",
    "    quantized = size_mb(QUANTIZED_PT_PATH)\n",
    "    print(f\"\\n원본 모델 크기: {orig:.2f} MB\")\n",
    "    print(f\"양자화 모델 크기: {quantized:.2f} MB\")\n",
    "    print(f\"압축률: {orig/quantized:.2f}x, 크기 감소: {(orig - quantized)/orig * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39cb4b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] tokenizer …\n",
      "[Load] quantized model from: /home/j-k13b204/S13P31B204/model_test/kobert_sentiment_model/quantize/model_quantized.pt\n",
      "=== Sentence-level ===\n",
      "- 계속 실패하면서 스스로가 싫어졌습니다.\n",
      "  -> pred: negative  probs={'uncertain': 3.248096982133575e-05, 'negative': 0.9999674558639526, 'positive': 1.708578736270283e-07}\n",
      "\n",
      "=== Paragraph-level (mean of probs) ===\n",
      "{'paragraph_pred_id': 1, 'paragraph_pred_label': 'negative', 'paragraph_probs': {'uncertain': 3.248096982133575e-05, 'negative': 0.9999674558639526, 'positive': 1.708578736270283e-07}}\n"
     ]
    }
   ],
   "source": [
    "# infer_quantized_kobert_sentiment.py\n",
    "# - 동적 양자화(.pt) 모델 로드하여 문장 단위 감정 추론\n",
    "# - CPU 전용 (quantized model은 CPU에서 효과적)\n",
    "\n",
    "import re, numpy as np, torch\n",
    "from kobert_transformers import get_tokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===== 0) 환경/라벨 =====\n",
    "DEVICE  = torch.device(\"cpu\")   # 동적 양자화 모델은 CPU 추론 권장\n",
    "MAX_LEN = 256\n",
    "\n",
    "LABEL2ID = {'uncertain': 0, 'negative': 1, 'positive': 2}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "MODEL_PATH = \"/home/j-k13b204/S13P31B204/model_test/kobert_sentiment_model/quantize/model_quantized.pt\"\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=3, dr_rate=0.3, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p=dr_rate) if dr_rate and dr_rate > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out[0][:, 0]\n",
    "        logits = self.classifier(self.dropout(pooled))\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "# ===== 1) 문장 분할 =====\n",
    "def split_sentences(paragraph: str):\n",
    "    lines = [s.strip() for s in paragraph.strip().splitlines() if s.strip()]\n",
    "    sents = []\n",
    "    for line in lines:\n",
    "        pieces = re.split(r'(?<=[\\.!?])\\s+|(?<=다\\.)\\s+|(?<=요\\.)\\s+|(?<=,)\\s+', line)\n",
    "        sents += [p.strip() for p in pieces if p and p.strip()]\n",
    "    return sents\n",
    "\n",
    "# ===== 2) 모델/토크나이저 로드 =====\n",
    "print(\"[Load] tokenizer …\")\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "print(f\"[Load] quantized model from: {MODEL_PATH}\")\n",
    "# 전체 모듈로 저장한 양자화 모델은 torch.load로 바로 불러와 사용\n",
    "model = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "# ===== 3) 예측 함수 =====\n",
    "@torch.no_grad()\n",
    "def predict_sentences(sent_list):\n",
    "    results, probs_all = [], []\n",
    "    for sent in sent_list:\n",
    "        enc = tokenizer(\n",
    "            sent,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        input_ids      = enc[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "        token_type_ids = enc.get(\"token_type_ids\", attention_mask.new_zeros(attention_mask.size())).to(DEVICE)\n",
    "\n",
    "        out = model(input_ids, attention_mask, token_type_ids)\n",
    "        # 양자화 모델의 forward가 (logits, loss) 또는 logits 하나만 반환할 수 있으니 안전 처리\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "        prob = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()  # [num_classes]\n",
    "        pred_id = int(prob.argmax())\n",
    "        results.append({\n",
    "            \"text\": sent,\n",
    "            \"pred_id\": pred_id,\n",
    "            \"pred_label\": ID2LABEL[pred_id],\n",
    "            \"probs\": {\n",
    "                \"uncertain\": float(prob[LABEL2ID['uncertain']]),\n",
    "                \"negative\":  float(prob[LABEL2ID['negative']]),\n",
    "                \"positive\":  float(prob[LABEL2ID['positive']]),\n",
    "            }\n",
    "        })\n",
    "        probs_all.append(prob)\n",
    "\n",
    "    para_summary = None\n",
    "    if probs_all:\n",
    "        mean_prob = np.stack(probs_all, axis=0).mean(axis=0)\n",
    "        para_summary = {\n",
    "            \"paragraph_pred_id\": int(mean_prob.argmax()),\n",
    "            \"paragraph_pred_label\": ID2LABEL[int(mean_prob.argmax())],\n",
    "            \"paragraph_probs\": {\n",
    "                \"uncertain\": float(mean_prob[LABEL2ID['uncertain']]),\n",
    "                \"negative\":  float(mean_prob[LABEL2ID['negative']]),\n",
    "                \"positive\":  float(mean_prob[LABEL2ID['positive']]),\n",
    "            }\n",
    "        }\n",
    "    return results, para_summary\n",
    "\n",
    "# ===== 4) 테스트 실행 =====\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = \"\"\"\n",
    "계속 실패하면서 스스로가 싫어졌습니다.\"\"\"\n",
    "    sents = split_sentences(paragraph)\n",
    "    sent_results, para_result = predict_sentences(sents)\n",
    "\n",
    "    print(\"=== Sentence-level ===\")\n",
    "    for r in sent_results:\n",
    "        print(f\"- {r['text']}\")\n",
    "        print(f\"  -> pred: {r['pred_label']}  probs={r['probs']}\")\n",
    "\n",
    "    print(\"\\n=== Paragraph-level (mean of probs) ===\")\n",
    "    print(para_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67600d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
