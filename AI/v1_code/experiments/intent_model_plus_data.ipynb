{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72ec4082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>expression</th>\n",
       "      <th>intent_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì•ì—ì„œë„ ë§ì”€ë“œë ¸ì§€ë§Œ ì €ëŠ” ì¤‘êµ­ì—ì„œ ì¼ ë…„ ì •ë„ ìœ í•™ ìƒí™œì„ í–ˆì—ˆìŠµë‹ˆë‹¤.</td>\n",
       "      <td>c_private</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì•„ ë„¤ ì €ëŠ” ì œ ë‚˜ë¦„ëŒ€ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œë²•ì€ ì–´ ì²« ë²ˆì§¸ë¡œëŠ” ë“±ì‚°ì„ ì¢‹ì•„í•©ë‹ˆë‹¤.</td>\n",
       "      <td>c_private</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì§ë¬´ ì§€ì‹ì€ ë³¸ì¸ì´ ì¡°ì§ì˜ ì¡´ì¬ê°ì„ ë‚˜íƒ€ë‚´ê³  ì—…ë¬´ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸...</td>\n",
       "      <td>c_value</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê·¸ë˜ì„œ ê²°êµ­ì€ ì§€ê¸ˆ ì „ì—­ì„ í•˜ê³  ì§€ê¸ˆ ì´ ìë¦¬ì—ì„œ ë©´ì ‘ì„ ë³´ê³  ìˆëŠ”ë° ì €ì˜ ì„ íƒì´ ...</td>\n",
       "      <td>c_sincere_co</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì–´ ì €ëŠ” ì„±ê²©ì´ ì— ì¡°ê¸ˆ ê¸‰í•œ ë¶€ë¶„ì´ ìˆì–´ìš”.</td>\n",
       "      <td>c_person</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    expression  \\\n",
       "0           ì•ì—ì„œë„ ë§ì”€ë“œë ¸ì§€ë§Œ ì €ëŠ” ì¤‘êµ­ì—ì„œ ì¼ ë…„ ì •ë„ ìœ í•™ ìƒí™œì„ í–ˆì—ˆìŠµë‹ˆë‹¤.     c_private   \n",
       "1        ì•„ ë„¤ ì €ëŠ” ì œ ë‚˜ë¦„ëŒ€ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œë²•ì€ ì–´ ì²« ë²ˆì§¸ë¡œëŠ” ë“±ì‚°ì„ ì¢‹ì•„í•©ë‹ˆë‹¤.     c_private   \n",
       "2  ì§ë¬´ ì§€ì‹ì€ ë³¸ì¸ì´ ì¡°ì§ì˜ ì¡´ì¬ê°ì„ ë‚˜íƒ€ë‚´ê³  ì—…ë¬´ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸...       c_value   \n",
       "3  ê·¸ë˜ì„œ ê²°êµ­ì€ ì§€ê¸ˆ ì „ì—­ì„ í•˜ê³  ì§€ê¸ˆ ì´ ìë¦¬ì—ì„œ ë©´ì ‘ì„ ë³´ê³  ìˆëŠ”ë° ì €ì˜ ì„ íƒì´ ...  c_sincere_co   \n",
       "4                          ì–´ ì €ëŠ” ì„±ê²©ì´ ì— ì¡°ê¸ˆ ê¸‰í•œ ë¶€ë¶„ì´ ìˆì–´ìš”.      c_person   \n",
       "\n",
       "    intent_group  \n",
       "0  Communication  \n",
       "1  Communication  \n",
       "2  Communication  \n",
       "3  Communication  \n",
       "4  Communication  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"/home/j-k13b204/S13P31B204/model_test/Code/csv_collection/intent_dataset_comm.csv\", encoding = \"utf-8\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfead102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intent_group\n",
       "Integrity              24479\n",
       "Job_Competency         23777\n",
       "Adaptability           22549\n",
       "Teamwork_Leadership    21706\n",
       "Communication          19963\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['intent_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4056227e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Communication',\n",
       " 'Teamwork_Leadership',\n",
       " 'Integrity',\n",
       " 'Adaptability',\n",
       " 'Job_Competency']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = data['intent_group'].unique().tolist()\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5985b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====ì‹œì‘=====\n",
      "intent_group: Communication, text: ì‚´ë©´ì„œ ê°€ì¥ í˜ë“¤ì—ˆë˜ ì ì€ ì•„ë§ˆ ì–´ë¦´ ë•Œë¶€í„° ê·¹ë³µí•  ìˆ˜ ì—†ì—ˆë˜ ì§€ë…í•œ ê°€ë‚œì´ì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Communication, text: ì–´ ê·¸ë˜ì„œ ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì¼ ë…„ ì •ë„ì˜ ì‹œê°„ ë™ì•ˆ ë™ê±°ë™ë½í•˜ë©´ì„œ ë°¤ì„ ìƒˆë©´ì„œ ì—´ì‹¬íˆ ê³µëª¨ì „ì„ ì¤€ë¹„ë¥¼ í•´ì„œ ëŒ€ìƒì´ë¼ëŠ” í° ìƒì„ ë°›ì•˜ê³  ê·¸ë•Œì˜ ê²½í—˜ê³¼ ì„±ê³¼ë“¤ë¡œ í†µí•´ì„œ ì œê°€ í•œì¸µ ì„±ì¥í•´ ë‚˜ê°”ë‹¤ëŠ” ê²ƒì„ ë§ì´ ëŠë‚„ ìˆ˜ ìˆì—ˆë˜ ë§¤ìš° ì†Œì¤‘í•œ ê²½í—˜ì´ì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Communication, text: ì–´ë–»ê²Œ ë³´ë©´ ì§„ì¤‘í•œ ì„±ê²©ì´ë¼ê³  í•  ìˆ˜ ìˆìœ¼ë‚˜ ì–´ë–¤ ì¼ì—ì„œë“ ì§€ ë„ˆë¬´ë‚˜ ì§„ì¤‘í•œ íƒ“ì— ì–´ë–¤ ê²ƒì„ ê²°ì •í•˜ê³  íŒë‹¨í•˜ëŠ” ë°ì— ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦¬ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
      "intent_group: Communication, text: í•˜ì§€ë§Œ ì´ì— ë§Œì¡±í•˜ì§€ ì•Šê³  ì €ëŠ” ë°± í‚¬ë¡œë¯¸í„° ìš¸íŠ¸ë¼ ë§ˆë¼í†¤ìœ¼ë¡œ ì™„ì£¼í•˜ê¸° ìœ„í•´ì„œ ëŠì„ì—†ì´ ë§¤ì¼ë§¤ì¼ ë‹¬ë¦¬ê¸° ì—°ìŠµì„ í•˜ì˜€ê³  ê·¸ ê²°ê³¼ ê²°êµ­ ëª‡ ë…„ ì „ì— ë°± í‚¬ë¡œë¯¸í„°ë¥¼ ì™„ì£¼í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Communication, text: í•™ì°½ ì‹œì ˆ ì‚¼ ë…„ ë‚´ë‚´ ì•„ë²„ì§€ë‘ ê±°ì˜ ë§ì„ í•˜ì§€ ì•Šì„ ì •ë„ë¡œ ê°™ì€ ì‹íƒì— ì•‰ê¸°ê°€ ì‹«ì–´ì„œ ì—¬ì„¯ ì‹œ ë˜ë©´ ì²« ì°¨ë¥¼ íƒ€ê³  ì§‘ì—ì„œ ë‚˜ì™€ì„œ ì—´ ì‹œ ì—´ ì‹œ ë°˜ê¹Œì§€ ì•„ë¥´ë°”ì´íŠ¸ë¥¼ í•˜ê³  ì§‘ì— ê·€ê°€í•œ ê¸°ì–µì´ ìˆì–´ìš”.\n",
      "=====ì‹œì‘=====\n",
      "intent_group: Teamwork_Leadership, text: í”„ë¡œì íŠ¸ ë¦¬ë”ë¡œ ê¸°íšÂ·ë””ìì¸Â·ê°œë°œ íŒ€ì´ ì„œë¡œ ì±…ì„ì„ ë¯¸ë£¨ë˜ ìƒí™©ì—ì„œ ì €ëŠ” ì—…ë¬´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë¶€ ê³µê°œí•˜ê³ , ë‚œì´ë„ì™€ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë°°í–ˆìŠµë‹ˆë‹¤. íšŒì˜ ì‹œê°„ì´ ì¤„ì–´ë“¤ê³ , ì‹¤í–‰ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì•„ì´ë””ì–´ê°€ ëŠ˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Teamwork_Leadership, text: ë°ì´í„° ë¶„ì„ ì¸í„´ìœ¼ë¡œ íšŒì˜ ë•Œë§ˆë‹¤ ëª‡ëª‡ íŒ€ì›ë§Œ ì˜ê²¬ì„ ë‚´ê³  ë‚˜ë¨¸ì§€ëŠ” ì¹¨ë¬µí•˜ë˜ ë¶„ìœ„ê¸°ì—ì„œ ì €ëŠ” ìš°ì„ ìˆœìœ„ë¥¼ ë‹¤ì‹œ ì •í•˜ê³ , í•„ìš” ì—†ëŠ” ì‘ì—…ì€ ê³¼ê°íˆ ì œì™¸í–ˆìŠµë‹ˆë‹¤. ì§‘ì¤‘í•´ì•¼ í•  ì‘ì—…ì— í˜ì„ ëª¨ì„ ìˆ˜ ìˆì–´ ì¼ì • ì•ˆì— ë§ˆë¬´ë¦¬í–ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Teamwork_Leadership, text: ì•Œë°” ì§ì›ìœ¼ë¡œ íŒ€ì˜ ëª©í‘œê°€ ëª…í™•í•˜ì§€ ì•Šì•„ ê°ì ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë˜ ì‹œì ì— ì €ëŠ” ìƒˆ íŒ€ì›ì—ê²Œ í”„ë¡œì íŠ¸ ë°°ê²½ê³¼ íŒ€ ë¬¸í™”ë¥¼ ë”°ë¡œ ì„¤ëª…í•´ ì£¼ë©° ì ì‘ì„ ë„ì™”ìŠµë‹ˆë‹¤. ì§‘ì¤‘í•´ì•¼ í•  ì‘ì—…ì— í˜ì„ ëª¨ì„ ìˆ˜ ìˆì–´ ì¼ì • ì•ˆì— ë§ˆë¬´ë¦¬í–ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Teamwork_Leadership, text: í”„ë¡œì íŠ¸ ë¦¬ë”ë¡œ ê¸°íšÂ·ë””ìì¸Â·ê°œë°œ íŒ€ì´ ì„œë¡œ ì±…ì„ì„ ë¯¸ë£¨ë˜ ìƒí™©ì—ì„œ ì €ëŠ” ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì •ì—ëŠ” ê´€ë ¨ì ëª¨ë‘ë¥¼ ì°¸ì—¬ì‹œí‚¤ëŠ” ë°©ì‹ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼ íŒ€ ë‚´ ë¶ˆë§Œì´ ì¤„ì–´ë“¤ê³  í˜‘ì—… ì†ë„ê°€ ë¹¨ë¼ì¡ŒìŠµë‹ˆë‹¤.\n",
      "intent_group: Teamwork_Leadership, text: CS ë‹´ë‹¹ìë¡œ íŒ€ì˜ ëª©í‘œê°€ ëª…í™•í•˜ì§€ ì•Šì•„ ê°ì ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë˜ ì‹œì ì— ì €ëŠ” ëª¨ë“  íŒ€ì›ì´ ëŒì•„ê°€ë©° ì˜ê²¬ì„ ë§í•˜ë„ë¡ ë¶„ìœ„ê¸°ë¥¼ ì¡°ì„±í–ˆìŠµë‹ˆë‹¤. íšŒì˜ ì‹œê°„ì´ ì¤„ì–´ë“¤ê³ , ì‹¤í–‰ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì•„ì´ë””ì–´ê°€ ëŠ˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "=====ì‹œì‘=====\n",
      "intent_group: Integrity, text: ì¸í„´ìœ¼ë¡œ ìƒëŒ€ íŒ€ì˜ ì‹¤ìˆ˜ë¥¼ ë³´ê³ ë„ ëª¨ë¥¸ ì²™í•˜ìëŠ” ì´ì•¼ê¸°ê°€ ë‚˜ì™”ì§€ë§Œ ì €ëŠ” ì›ì²œ ë°ì´í„°ë¶€í„° ë‹¤ì‹œ ê²€ì¦í•´ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ë§ì¶˜ ë’¤ ë³´ê³ í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼ ì‹ ë¢°ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Integrity, text: í”„ë¡œì íŠ¸ ë¦¬ë”ë¡œ ê³ ê°ì—ê²Œ ë¶ˆë¦¬í•  ìˆ˜ ìˆëŠ” ìˆ˜ìˆ˜ë£Œ êµ¬ì¡°ë¥¼ ìˆ¨ê¸°ìëŠ” ë¶„ìœ„ê¸°ê°€ ìˆì—ˆì§€ë§Œ ì €ëŠ” ê³ ê° ì…ì¥ì—ì„œ ë¶ˆë¦¬í•œ ì ì„ ë¨¼ì € ì„¤ëª…í•˜ê³  ëŒ€ì•ˆì„ í•¨ê»˜ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ë¹„ë¡ ì‹œê°„ì´ ë” ê±¸ë ¸ì§€ë§Œ ì¥ê¸°ì ìœ¼ë¡œ ì¢‹ì€ í‰ê°€ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "intent_group: Integrity, text: ì„œë¹„ìŠ¤ ê¸°íšìë¡œ í´ë ˆì„ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ë‚´ìš©ì„ ë³´ê³ ì„œì— ì“°ìëŠ” ì˜ê²¬ì´ ë‚˜ì™”ì§€ë§Œ ì €ëŠ” ì‹¤ìˆ˜ë¥¼ ì¸ì •í•˜ê³  í•¨ê»˜ í•´ê²°ì±…ì„ ì°¾ìëŠ” ë°©í–¥ìœ¼ë¡œ íšŒì˜ë¥¼ ì´ëŒì—ˆìŠµë‹ˆë‹¤. ì´í›„ì—ëŠ” ìœ ì‚¬í•œ ìƒí™©ì—ì„œë„ ë” ì¹¨ì°©í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Integrity, text: ê³ ê°ì—ê²Œ ì§‘ì¤‘í•˜ê³  ìˆìœ¼ë©´ ê³ ê°ì´ ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ ê·¸ë¦¬ê³  ì™œ ê·¸ê²ƒì„ ìš”êµ¬í•˜ëŠ”ì§€ê¹Œì§€ë„ ìƒê°í•  ìˆ˜ ìˆëŠ” ë§Œí¼ ê´€ì‹¬ê³¼ ë°°ë ¤ë¥¼ ê°–ê²Œ ë©ë‹ˆë‹¤.\n",
      "intent_group: Integrity, text: ì‹ ì… ì‚¬ì›ìœ¼ë¡œ ë§¤ì¶œ ë°ì´í„°ê°€ ë§ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ëŒ€ì¶© ë„˜ê¸°ìëŠ” ì œì•ˆì´ ìˆì—ˆì§€ë§Œ ì €ëŠ” ê³ ê° ì…ì¥ì—ì„œ ë¶ˆë¦¬í•œ ì ì„ ë¨¼ì € ì„¤ëª…í•˜ê³  ëŒ€ì•ˆì„ í•¨ê»˜ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ë‹¨ê¸° ì„±ê³¼ëŠ” ì¤„ì—ˆì§€ë§Œ, íŒ€ê³¼ íšŒì‚¬ì˜ ì‹ ë¢°ë„ëŠ” ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤.\n",
      "=====ì‹œì‘=====\n",
      "intent_group: Adaptability, text: ì¸í„´ìœ¼ë¡œ ê³ ê° ìš”êµ¬ì‚¬í•­ì´ í¬ê²Œ ë°”ë€Œì–´ ê¸°íšì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•´ì•¼ í–ˆì„ ë•Œ ì €ëŠ” ì„ ë°°ì—ê²Œ íˆìŠ¤í† ë¦¬ë¥¼ ë¨¼ì € ë“£ê³  ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ í•˜ë‚˜ì”© ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ë‹¹ì‹œì—ëŠ” í˜ë“¤ì—ˆì§€ë§Œ ë‚˜ì¤‘ì—ëŠ” ì˜¤íˆë ¤ ê°ì‚¬ ì¸ì‚¬ë¥¼ ë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Adaptability, text: ì˜ì—… ì‚¬ì›ìœ¼ë¡œ ë‹´ë‹¹ìê°€ í‡´ì‚¬í•˜ë©´ì„œ ì œê°€ ì—…ë¬´ë¥¼ ì¸ìˆ˜ë°›ê²Œ ë˜ì—ˆì„ ë•Œ ì €ëŠ” ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•œ ë’¤, ê¸°ì¡´ êµ¬ì¡°ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ë¶„ì„ ë¨¼ì € ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´í›„ì—ëŠ” ìœ ì‚¬í•œ ìƒí™©ì—ì„œë„ ë” ì¹¨ì°©í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Adaptability, text: ì•Œë°” ì§ì›ìœ¼ë¡œ í”„ë¡œì íŠ¸ ì¼ì •ì´ ë‹¹ê²¨ì ¸ ì—¬ìœ ê°€ ê±°ì˜ ì‚¬ë¼ì¡Œì„ ë•Œ ì €ëŠ” ì„ ë°°ì—ê²Œ íˆìŠ¤í† ë¦¬ë¥¼ ë¨¼ì € ë“£ê³  ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ í•˜ë‚˜ì”© ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ì´í›„ì—ëŠ” ìœ ì‚¬í•œ ìƒí™©ì—ì„œë„ ë” ì¹¨ì°©í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Adaptability, text: ë°ì´í„° ë¶„ì„ ì¸í„´ìœ¼ë¡œ í”„ë¡œì íŠ¸ ì¼ì •ì´ ë‹¹ê²¨ì ¸ ì—¬ìœ ê°€ ê±°ì˜ ì‚¬ë¼ì¡Œì„ ë•Œ ì €ëŠ” ì¼ì •ì„ ë‹¤ì‹œ ìª¼ê°œì–´ í•„ìˆ˜ ì—…ë¬´ì™€ ì„ íƒ ì—…ë¬´ë¥¼ êµ¬ë¶„í–ˆìŠµë‹ˆë‹¤. ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì—ˆê³  ê°™ì€ ë¬¸ì œê°€ ë°˜ë³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
      "intent_group: Adaptability, text: CS ë‹´ë‹¹ìë¡œ íŒ€ êµ¬ì¡°ê°€ ê°œí¸ë˜ì–´ ì „í˜€ ë‹¤ë¥¸ ì¡°ì§ìœ¼ë¡œ ì´ë™í•˜ê²Œ ë˜ì—ˆì„ ë•Œ ì €ëŠ” ìš°ì„ ìˆœìœ„ë¥¼ ì¬ì¡°ì •í•˜ê³ , í•„ìš”í•œ ê²½ìš° ì´í•´ê´€ê³„ìì™€ ì¼ì •ì„ ì¬í˜‘ì˜í–ˆìŠµë‹ˆë‹¤. ë‹¨ê¸° ì„±ê³¼ëŠ” ì¤„ì—ˆì§€ë§Œ, íŒ€ê³¼ íšŒì‚¬ì˜ ì‹ ë¢°ë„ëŠ” ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤.\n",
      "=====ì‹œì‘=====\n",
      "intent_group: Job_Competency, text: ì´ëŸ° ë¶„ì„ì´ ê¸°ë°˜ì´ ë¼ì•¼ë§Œ ìœ  ì—‘ìŠ¤ ë””ìì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì œí’ˆì„ ì‚¬ìš©í•  ì‚¬ëŒë“¤ì—ê²Œ ê°€ì¹˜ ìˆëŠ” ì œí’ˆì„   ì œê³µí•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n",
      "intent_group: Job_Competency, text: ë¯¸ë˜ì—ëŠ” ì•„ì´ì”¨í‹° ë¶„ì•¼ì— ì–´ ì„œ ê°œë°œë˜ëŠ” ì–´ë–¤ í”„ë¡œê·¸ë¨ ì œí’ˆ ë˜ëŠ” ì–´ë–¤ ì´ ì¥ì¹˜ë“¤ ì´ëŸ° ê²ƒë“¤ì´ ë¬´ìˆ˜íˆ ë§ì´ í•„ìš”ë¡œ í•˜ëŠ” ì‹œëŒ€ê°€ ë‹¤ê°€ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "intent_group: Job_Competency, text: ì €ëŠ” ìƒì‚° ê´€ë¦¬ íŒ€ì— ìˆìœ¼ë©´ì„œ ì¼ì„ ì—´ì‹¬íˆ ì†ë„ë¥¼ ë†’ì—¬ì„œ í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•˜ì§€ë§Œ ë‚©ê¸°ë¥¼ ë§ì¶”ê³  ìƒì‚° ê¸°í•œ ì•ˆì— ëª©í‘œí•œ ë§Œí¼ì˜ ìƒì‚°ì„ ë§‰ ìƒì‚°ëŸ‰ì„ ë§ì¶”ëŠ” ê²ƒì€ ì „ëµ ê¸°íšì´ë‚˜ ì•„ë‹ˆë©´ ì „ì²´ì ì¸ ì¼ì˜ ì „ì²´ì ì¸ ê²ƒì„ í†µê³„ë¥¼ ë‚´ê±°ë‚˜ ì´ëŸ° ê²ƒë“¤ì„ ìˆ˜ì¹˜í™”í•´ì„œ ì •í™•í•˜ê²Œ ì¸¡ì •í•˜ê³  ì–´ë””ì—ì„œ ë¶€ì¡±í•œ ë¶€ë¶„ì´ ìƒê¸°ê³  ë¬´ì—‡ì´ ë¦¬ìŠ¤ê°€ ë‚˜ëŠ” ê²ƒì¸ì§€ í™•ì¸í•˜ëŠ” ì‘ì—…ë“¤ì´ ë¶„ëª…íˆ í•„ìš”í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n",
      "intent_group: Job_Competency, text: ë°ì´í„° ë¶„ì„ ì¸í„´ìœ¼ë¡œ ë§¤ì¶œì´ ê°ì†Œí•˜ë˜ ì‹œê¸°ì— ì—¬ëŸ¬ ì§€í‘œë¥¼ ì¢…í•©í•´ ìº í˜ì¸ ì„±ê³¼ë¥¼ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì—ˆê³  ê°™ì€ ë¬¸ì œê°€ ë°˜ë³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
      "intent_group: Job_Competency, text: ì„œë¹„ìŠ¤ ê¸°íšìë¡œ ì‹ ê·œ ì„œë¹„ìŠ¤ ì˜¤í”ˆ ì¤€ë¹„ ê³¼ì •ì—ì„œ SQLê³¼ íŒŒì´ì¬ì„ ì‚¬ìš©í•´ ë°˜ë³µë˜ëŠ” ë¦¬í¬íŠ¸ ì‘ì—…ì„ ìë™í™”í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²½í—˜ì„ í†µí•´ í•œ ë‹¨ê³„ ì„±ì¥í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "for g in groups : \n",
    "    print(\"=====ì‹œì‘=====\")\n",
    "    sample_rows = data[data['intent_group'] == g].sample(5, random_state=42)\n",
    "    for idx, row in sample_rows.iterrows() :\n",
    "        print(f\"intent_group: {row['intent_group']}, text: {row['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c017e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>expression</th>\n",
       "      <th>intent_group</th>\n",
       "      <th>label</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì•ì—ì„œë„ ë§ì”€ë“œë ¸ì§€ë§Œ ì €ëŠ” ì¤‘êµ­ì—ì„œ ì¼ ë…„ ì •ë„ ìœ í•™ ìƒí™œì„ í–ˆì—ˆìŠµë‹ˆë‹¤.</td>\n",
       "      <td>c_private</td>\n",
       "      <td>Communication</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì•„ ë„¤ ì €ëŠ” ì œ ë‚˜ë¦„ëŒ€ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œë²•ì€ ì–´ ì²« ë²ˆì§¸ë¡œëŠ” ë“±ì‚°ì„ ì¢‹ì•„í•©ë‹ˆë‹¤.</td>\n",
       "      <td>c_private</td>\n",
       "      <td>Communication</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì§ë¬´ ì§€ì‹ì€ ë³¸ì¸ì´ ì¡°ì§ì˜ ì¡´ì¬ê°ì„ ë‚˜íƒ€ë‚´ê³  ì—…ë¬´ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸...</td>\n",
       "      <td>c_value</td>\n",
       "      <td>Communication</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê·¸ë˜ì„œ ê²°êµ­ì€ ì§€ê¸ˆ ì „ì—­ì„ í•˜ê³  ì§€ê¸ˆ ì´ ìë¦¬ì—ì„œ ë©´ì ‘ì„ ë³´ê³  ìˆëŠ”ë° ì €ì˜ ì„ íƒì´ ...</td>\n",
       "      <td>c_sincere_co</td>\n",
       "      <td>Communication</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì–´ ì €ëŠ” ì„±ê²©ì´ ì— ì¡°ê¸ˆ ê¸‰í•œ ë¶€ë¶„ì´ ìˆì–´ìš”.</td>\n",
       "      <td>c_person</td>\n",
       "      <td>Communication</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    expression  \\\n",
       "0           ì•ì—ì„œë„ ë§ì”€ë“œë ¸ì§€ë§Œ ì €ëŠ” ì¤‘êµ­ì—ì„œ ì¼ ë…„ ì •ë„ ìœ í•™ ìƒí™œì„ í–ˆì—ˆìŠµë‹ˆë‹¤.     c_private   \n",
       "1        ì•„ ë„¤ ì €ëŠ” ì œ ë‚˜ë¦„ëŒ€ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œë²•ì€ ì–´ ì²« ë²ˆì§¸ë¡œëŠ” ë“±ì‚°ì„ ì¢‹ì•„í•©ë‹ˆë‹¤.     c_private   \n",
       "2  ì§ë¬´ ì§€ì‹ì€ ë³¸ì¸ì´ ì¡°ì§ì˜ ì¡´ì¬ê°ì„ ë‚˜íƒ€ë‚´ê³  ì—…ë¬´ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸...       c_value   \n",
       "3  ê·¸ë˜ì„œ ê²°êµ­ì€ ì§€ê¸ˆ ì „ì—­ì„ í•˜ê³  ì§€ê¸ˆ ì´ ìë¦¬ì—ì„œ ë©´ì ‘ì„ ë³´ê³  ìˆëŠ”ë° ì €ì˜ ì„ íƒì´ ...  c_sincere_co   \n",
       "4                          ì–´ ì €ëŠ” ì„±ê²©ì´ ì— ì¡°ê¸ˆ ê¸‰í•œ ë¶€ë¶„ì´ ìˆì–´ìš”.      c_person   \n",
       "\n",
       "    intent_group  label  label_0  label_1  label_2  label_3  label_4  \n",
       "0  Communication      0      1.0      0.0      0.0      0.0      0.0  \n",
       "1  Communication      0      1.0      0.0      0.0      0.0      0.0  \n",
       "2  Communication      0      1.0      0.0      0.0      0.0      0.0  \n",
       "3  Communication      0      1.0      0.0      0.0      0.0      0.0  \n",
       "4  Communication      0      1.0      0.0      0.0      0.0      0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) ë¼ë²¨ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬\n",
    "label_dict = {\n",
    "    'Communication': 0,\n",
    "    'Teamwork_Leadership': 1,\n",
    "    'Integrity': 2,\n",
    "    'Adaptability': 3,\n",
    "    'Job_Competency': 4\n",
    "}\n",
    "\n",
    "# 2) ì •ìˆ˜ ë¼ë²¨ ìƒì„±\n",
    "data['label'] = data['intent_group'].map(label_dict)\n",
    "\n",
    "# 3) ì›-í•« ì¸ì½”ë”© ìƒì„±\n",
    "num_classes = len(label_dict)    # = 5\n",
    "labels = data['label'].values    # ì •ìˆ˜ë¼ë²¨ ë°°ì—´\n",
    "\n",
    "one_hot = np.eye(num_classes)[labels]   # ì›-í•« ë²¡í„°\n",
    "\n",
    "# 4) DataFrameìœ¼ë¡œ ë³€í™˜ í›„ ì›ë³¸ ë°ì´í„°ì— ë³‘í•©\n",
    "one_hot_df = pd.DataFrame(\n",
    "    one_hot, \n",
    "    columns=[f\"label_{i}\" for i in range(num_classes)]\n",
    ")\n",
    "\n",
    "data = pd.concat([data, one_hot_df], axis=1)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3d66f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW    \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup \n",
    "\n",
    "from kobert_transformers import get_kobert_model, get_tokenizer\n",
    "\n",
    "# BERT ëª¨ë¸/í† í¬ë‚˜ì´ì €\n",
    "bertmodel = get_kobert_model()\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57959ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS   = 5     \n",
    "MAX_LEN      = 256\n",
    "BATCH_SIZE   = 32\n",
    "EPOCHS       = 10\n",
    "LR           = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "DROPOUT      = 0.5\n",
    "PATIENCE     = 2\n",
    "WARMUP_RATIO = 0.06\n",
    "MAX_GRAD_NORM = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebdc5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "# MAX_LEN ì•ˆì „ì„ \n",
    "assert MAX_LEN <= 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f56231ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBertClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert,\n",
    "        hidden_size: int = 768,\n",
    "        num_classes: int = 5,\n",
    "        dr_rate: float = 0.3,\n",
    "        pos_weight: torch.Tensor | None = None  # ê° í´ë˜ìŠ¤ì˜ ì–‘ì„± ê°€ì¤‘ì¹˜ (ë¶ˆê· í˜• ë³´ì •)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p=dr_rate) if dr_rate and dr_rate > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # BCEWithLogitsLoss: ë‚´ë¶€ì—ì„œ sigmoid + binary cross entropy ìˆ˜í–‰\n",
    "        if pos_weight is not None:\n",
    "            # pos_weight: ê° í´ë˜ìŠ¤ì— ëŒ€í•´ (neg/pos ë¹„ìœ¨) í˜•íƒœ ê¶Œì¥\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        else:\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor | None = None,\n",
    "        token_type_ids: torch.Tensor | None = None,\n",
    "        labels: torch.Tensor | None = None  # (B, num_classes), multi-hot (0/1)\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        # pooler_output ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ [CLS] í† í° ì‚¬ìš©\n",
    "        if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            pooled = outputs.pooler_output                  # (B, 768)\n",
    "        else:\n",
    "            pooled = outputs[0][:, 0]                       # last_hidden_state[:, 0]\n",
    "\n",
    "        logits = self.classifier(self.dropout(pooled))      # (B, num_classes), ìƒ ë¡œì§“\n",
    "\n",
    "        if labels is not None:\n",
    "            # BCEWithLogitsLossëŠ” float íƒ€ê²Ÿ ê¸°ëŒ€\n",
    "            labels = labels.float()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            return probs, loss\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aeb2bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECK ===\n",
      "Train labels_mat shape: (78731, 5)\n",
      "Valid labels_mat shape: (33743, 5)\n",
      "Positive counts per class: [13986. 15313. 17083. 15798. 16551.]\n",
      "Negative counts per class: [64745. 63418. 61648. 62933. 62180.]\n",
      "pos_weight: tensor([4.6293, 4.1414, 3.6087, 3.9836, 3.7569], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 5  \n",
    "\n",
    "# 1ï¸âƒ£ textë§Œ NaN ì•„ë‹ˆë©´ OK (label, label_*ëŠ” ì´ë¯¸ ìˆë‹¤ê³  ê°€ì •)\n",
    "clean_df = data.dropna(subset=['text'])\n",
    "\n",
    "# 2ï¸âƒ£ Train / Valid split\n",
    "train_df, valid_df = train_test_split(\n",
    "    clean_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# 3ï¸âƒ£ ì›-í•« ì¸ì½”ë”© ì»¬ëŸ¼ì—ì„œ (N, 5) ë¼ë²¨ í–‰ë ¬ ë§Œë“¤ê¸°\n",
    "label_cols = [f'label_{i}' for i in range(NUM_CLASSES)]\n",
    "\n",
    "train_labels_mat = train_df[label_cols].values.astype(np.float32)  # (N_train, 5)\n",
    "valid_labels_mat = valid_df[label_cols].values.astype(np.float32)  # (N_valid, 5)\n",
    "\n",
    "# 4ï¸âƒ£ pos_weight ê³„ì‚° (Train ê¸°ì¤€)\n",
    "pos_counts = train_labels_mat.sum(axis=0)                      # ê° í´ë˜ìŠ¤ì—ì„œ 1ì˜ ê°œìˆ˜\n",
    "neg_counts = train_labels_mat.shape[0] - pos_counts            # ê° í´ë˜ìŠ¤ì—ì„œ 0ì˜ ê°œìˆ˜\n",
    "\n",
    "# í˜¹ì‹œ íŠ¹ì • í´ë˜ìŠ¤ì— ì–‘ì„±ì´ 0ê°œì¸ ê²½ìš° ëŒ€ë¹„\n",
    "pos_counts_safe = np.where(pos_counts == 0, 1e-8, pos_counts)\n",
    "\n",
    "pos_weight = torch.tensor(neg_counts / pos_counts_safe, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"=== CHECK ===\")\n",
    "print(\"Train labels_mat shape:\", train_labels_mat.shape)\n",
    "print(\"Valid labels_mat shape:\", valid_labels_mat.shape)\n",
    "print(\"Positive counts per class:\", pos_counts)\n",
    "print(\"Negative counts per class:\", neg_counts)\n",
    "print(\"pos_weight:\", pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ece4c2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== pos_weight ë§¤í•‘ ê²°ê³¼ ===\n",
      "0: Communication -> pos_weight = 4.6293\n",
      "1: Teamwork_Leadership -> pos_weight = 4.1414\n",
      "2: Integrity -> pos_weight = 3.6087\n",
      "3: Adaptability -> pos_weight = 3.9836\n",
      "4: Job_Competency -> pos_weight = 3.7569\n"
     ]
    }
   ],
   "source": [
    "# pos_weightê°€ ì´ë¯¸ tensorë¡œ ì¡´ì¬í•œë‹¤ê³  ê°€ì •\n",
    "# ì˜ˆ: pos_weight = tensor([0.76, 4.39, 11.57, 23.10, 6.84])\n",
    "\n",
    "idx_to_label = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "print(\"=== pos_weight ë§¤í•‘ ê²°ê³¼ ===\")\n",
    "for idx, weight in enumerate(pos_weight.cpu().numpy()):\n",
    "    label_name = idx_to_label[idx]\n",
    "    print(f\"{idx}: {label_name} -> pos_weight = {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "318de657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "SAVE_DIR = \"/home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2.1\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "THRESHOLD = 0.5  # í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ë‚˜ì¤‘ì— íŠœë‹ ê°€ëŠ¥\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train\"):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device, non_blocking=True)\n",
    "        labels         = batch[\"labels\"].to(device, non_blocking=True)  # (B, 5) float\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs, loss = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        all_probs.append(probs.detach().cpu().numpy())   # (B, 5)\n",
    "        all_labels.append(labels.detach().cpu().numpy()) # (B, 5)\n",
    "    \n",
    "    all_probs  = np.vstack(all_probs)   # (N, 5)\n",
    "    all_labels = np.vstack(all_labels)  # (N, 5)\n",
    "\n",
    "    # ğŸ”¹ ë©€í‹°ë¼ë²¨: thresholdë¡œ 0/1 ì˜ˆì¸¡\n",
    "    y_true = all_labels.astype(int)                         # (N, 5)\n",
    "    y_pred = (all_probs >= THRESHOLD).astype(int)           # (N, 5)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)                    # multilabel indicator ì§€ì›\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    return total_loss / len(loader), acc, macro_f1\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, target_names=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Valid\"):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device, non_blocking=True)\n",
    "        labels         = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        probs, loss = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        if loss is not None:\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "    \n",
    "    all_probs  = np.vstack(all_probs)  \n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    y_true = all_labels.astype(int)               \n",
    "    y_pred = (all_probs >= THRESHOLD).astype(int) \n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    rep = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=target_names,\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    cm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return total_loss / len(loader), acc, macro_f1, rep, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee56519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def save_confusion_matrix(cm, label_names, save_dir, epoch):\n",
    "    \"\"\"\n",
    "    cm : multilabel_confusion_matrix ê²°ê³¼ (shape: (num_labels, 2, 2))\n",
    "    label_names : ['Communication', ...]\n",
    "    \"\"\"\n",
    "    for i, name in enumerate(label_names):\n",
    "        matrix = cm[i]  # (2,2) = [[TN, FP], [FN, TP]]\n",
    "\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(\n",
    "            matrix,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "            yticklabels=[\"True 0\", \"True 1\"]\n",
    "        )\n",
    "        plt.title(f\"Confusion Matrix - {name} (Epoch {epoch})\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"cm_{name}_epoch{epoch}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ba87cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 24610, Warmup steps: 1476\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì • (ì´ë¯¸ ìœ„ì—ì„œ ì„ ì–¸ë˜ì–´ ìˆë‹¤ë©´ ìƒëµ)\n",
    "# EPOCHS, WARMUP_RATIO, BATCH_SIZE ë“±\n",
    "\n",
    "# ì „ì²´ ìŠ¤í… ìˆ˜ = (ì—í­ ìˆ˜) Ã— (ì—í­ë‹¹ ë°°ì¹˜ ìˆ˜)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05987e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scheduler is not None:\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ebd64a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== EPOCH 1/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   1%|â–ˆâ–                                                                                                         | 33/2461 [00:14<18:01,  2.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m==== EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ====\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     tr_loss, tr_acc, tr_f1 = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     val_loss, val_acc, val_f1, val_rep = evaluate(\n\u001b[32m     27\u001b[39m         model,\n\u001b[32m     28\u001b[39m         valid_loader,\n\u001b[32m     29\u001b[39m         target_names=label_names\n\u001b[32m     30\u001b[39m     )\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Train] loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m macro_f1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, scheduler)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m     scheduler.step()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m all_probs.append(probs.detach().cpu().numpy())   \u001b[38;5;66;03m# (B, 5)\u001b[39;00m\n\u001b[32m     43\u001b[39m all_labels.append(labels.detach().cpu().numpy()) \u001b[38;5;66;03m# (B, 5)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"/home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2.1\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "label_dict = {\n",
    "    'Communication': 0,\n",
    "    'Teamwork_Leadership': 1,\n",
    "    'Integrity': 2,\n",
    "    'Adaptability': 3,\n",
    "    'Job_Competency': 4\n",
    "}\n",
    "\n",
    "label_names = [k for k, _ in sorted(label_dict.items(), key=lambda x: x[1])]\n",
    "\n",
    "best_f1, patience = 0.0, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n==== EPOCH {epoch}/{EPOCHS} ====\")\n",
    "    \n",
    "    tr_loss, tr_acc, tr_f1 = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc, val_f1, val_rep = evaluate(\n",
    "        model,\n",
    "        valid_loader,\n",
    "        target_names=label_names\n",
    "    )\n",
    "\n",
    "    print(f\"[Train] loss={tr_loss:.4f} acc={tr_acc:.4f} macro_f1={tr_f1:.4f}\")\n",
    "    print(f\"[Valid] loss={val_loss:.4f} acc={val_acc:.4f} macro_f1={val_f1:.4f}\")\n",
    "    print(val_rep)\n",
    "\n",
    "    # Early Stop ê¸°ì¤€: macro F1\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1, patience = val_f1, 0\n",
    "\n",
    "        save_path = os.path.join(SAVE_DIR, \"model_best.pt\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # ë¼ë²¨ ë§µ ì €ì¥\n",
    "        with open(os.path.join(SAVE_DIR, \"label_map.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            for k, v in sorted(label_dict.items(), key=lambda x: x[1]):\n",
    "                f.write(f\"{v}\\t{k}\\n\")\n",
    "\n",
    "        print(f\">>> Best saved. macroF1={best_f1:.4f} @ {save_path}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"patience {patience}/{PATIENCE}\")\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72a969d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 24610, Warmup steps: 1476\n",
      "\n",
      "==== EPOCH 1/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2461/2461 [17:36<00:00,  2.33it/s]\n",
      "Valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1055/1055 [02:20<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.2410 acc=0.8926 macro_f1=0.9206\n",
      "[Valid] loss=0.1035 acc=0.9354 macro_f1=0.9517\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      Communication     0.8852    0.9495    0.9162      5977\n",
      "Teamwork_Leadership     0.9764    0.9789    0.9777      6393\n",
      "          Integrity     0.9544    0.9790    0.9666      7396\n",
      "       Adaptability     0.9565    0.9716    0.9640      6751\n",
      "     Job_Competency     0.8897    0.9830    0.9340      7226\n",
      "\n",
      "          micro avg     0.9316    0.9731    0.9519     33743\n",
      "          macro avg     0.9324    0.9724    0.9517     33743\n",
      "       weighted avg     0.9329    0.9731    0.9523     33743\n",
      "        samples avg     0.9540    0.9731    0.9603     33743\n",
      "\n",
      ">>> Best saved. macroF1=0.9517 @ /home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2.1/model_best.pt\n",
      "\n",
      "==== EPOCH 2/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2461/2461 [17:34<00:00,  2.33it/s]\n",
      "Valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1055/1055 [02:19<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.0903 acc=0.9454 macro_f1=0.9577\n",
      "[Valid] loss=0.0874 acc=0.9440 macro_f1=0.9572\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      Communication     0.8537    0.9756    0.9106      5977\n",
      "Teamwork_Leadership     0.9604    0.9859    0.9730      6393\n",
      "          Integrity     0.9489    0.9842    0.9662      7396\n",
      "       Adaptability     0.9844    0.9610    0.9726      6751\n",
      "     Job_Competency     0.9537    0.9743    0.9639      7226\n",
      "\n",
      "          micro avg     0.9402    0.9762    0.9579     33743\n",
      "          macro avg     0.9402    0.9762    0.9572     33743\n",
      "       weighted avg     0.9423    0.9762    0.9584     33743\n",
      "        samples avg     0.9599    0.9762    0.9653     33743\n",
      "\n",
      ">>> Best saved. macroF1=0.9572 @ /home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2.1/model_best.pt\n",
      "\n",
      "==== EPOCH 3/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2461/2461 [17:35<00:00,  2.33it/s]\n",
      "Valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1055/1055 [02:20<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.0609 acc=0.9634 macro_f1=0.9718\n",
      "[Valid] loss=0.1044 acc=0.9452 macro_f1=0.9560\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      Communication     0.9146    0.9269    0.9207      5977\n",
      "Teamwork_Leadership     0.9631    0.9867    0.9747      6393\n",
      "          Integrity     0.9485    0.9830    0.9654      7396\n",
      "       Adaptability     0.9702    0.9707    0.9705      6751\n",
      "     Job_Competency     0.9174    0.9826    0.9489      7226\n",
      "\n",
      "          micro avg     0.9426    0.9712    0.9567     33743\n",
      "          macro avg     0.9428    0.9700    0.9560     33743\n",
      "       weighted avg     0.9429    0.9712    0.9567     33743\n",
      "        samples avg     0.9581    0.9712    0.9624     33743\n",
      "\n",
      "patience 1/2\n",
      "\n",
      "==== EPOCH 4/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2461/2461 [17:35<00:00,  2.33it/s]\n",
      "Valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1055/1055 [02:20<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.0394 acc=0.9795 macro_f1=0.9840\n",
      "[Valid] loss=0.1186 acc=0.9567 macro_f1=0.9616\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      Communication     0.8987    0.9511    0.9242      5977\n",
      "Teamwork_Leadership     0.9645    0.9864    0.9753      6393\n",
      "          Integrity     0.9746    0.9744    0.9745      7396\n",
      "       Adaptability     0.9590    0.9744    0.9666      6751\n",
      "     Job_Competency     0.9706    0.9640    0.9673      7226\n",
      "\n",
      "          micro avg     0.9547    0.9703    0.9625     33743\n",
      "          macro avg     0.9535    0.9701    0.9616     33743\n",
      "       weighted avg     0.9553    0.9703    0.9626     33743\n",
      "        samples avg     0.9635    0.9703    0.9657     33743\n",
      "\n",
      ">>> Best saved. macroF1=0.9616 @ /home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2.1/model_best.pt\n",
      "\n",
      "==== EPOCH 5/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2461/2461 [17:35<00:00,  2.33it/s]\n",
      "Valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1055/1055 [02:20<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.0235 acc=0.9895 macro_f1=0.9916\n",
      "[Valid] loss=0.1402 acc=0.9605 macro_f1=0.9635\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      Communication     0.9051    0.9496    0.9268      5977\n",
      "Teamwork_Leadership     0.9794    0.9822    0.9808      6393\n",
      "          Integrity     0.9726    0.9742    0.9734      7396\n",
      "       Adaptability     0.9708    0.9705    0.9707      6751\n",
      "     Job_Competency     0.9634    0.9679    0.9656      7226\n",
      "\n",
      "          micro avg     0.9591    0.9693    0.9642     33743\n",
      "          macro avg     0.9583    0.9689    0.9635     33743\n",
      "       weighted avg     0.9596    0.9693    0.9643     33743\n",
      "        samples avg     0.9649    0.9693    0.9663     33743\n",
      "\n",
      ">>> Best saved. macroF1=0.9635 @ /home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2.1/model_best.pt\n",
      "\n",
      "==== EPOCH 6/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2461/2461 [17:35<00:00,  2.33it/s]\n",
      "Valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1055/1055 [02:20<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.0138 acc=0.9942 macro_f1=0.9954\n",
      "[Valid] loss=0.1727 acc=0.9606 macro_f1=0.9626\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      Communication     0.9214    0.9297    0.9255      5977\n",
      "Teamwork_Leadership     0.9827    0.9798    0.9813      6393\n",
      "          Integrity     0.9664    0.9776    0.9720      7396\n",
      "       Adaptability     0.9652    0.9736    0.9694      6751\n",
      "     Job_Competency     0.9602    0.9690    0.9646      7226\n",
      "\n",
      "          micro avg     0.9599    0.9669    0.9634     33743\n",
      "          macro avg     0.9592    0.9659    0.9626     33743\n",
      "       weighted avg     0.9600    0.9669    0.9634     33743\n",
      "        samples avg     0.9637    0.9669    0.9648     33743\n",
      "\n",
      "patience 1/2\n",
      "\n",
      "==== EPOCH 7/10 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2461/2461 [17:36<00:00,  2.33it/s]\n",
      "Valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1055/1055 [02:20<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] loss=0.0072 acc=0.9968 macro_f1=0.9975\n",
      "[Valid] loss=0.2065 acc=0.9613 macro_f1=0.9623\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      Communication     0.9145    0.9287    0.9216      5977\n",
      "Teamwork_Leadership     0.9773    0.9828    0.9800      6393\n",
      "          Integrity     0.9690    0.9751    0.9720      7396\n",
      "       Adaptability     0.9706    0.9699    0.9703      6751\n",
      "     Job_Competency     0.9711    0.9640    0.9676      7226\n",
      "\n",
      "          micro avg     0.9616    0.9649    0.9633     33743\n",
      "          macro avg     0.9605    0.9641    0.9623     33743\n",
      "       weighted avg     0.9617    0.9649    0.9633     33743\n",
      "        samples avg     0.9631    0.9649    0.9637     33743\n",
      "\n",
      "patience 2/2\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# =========================\n",
    "# ê¸°ë³¸ ì„¤ì • / í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "THRESHOLD = 0.5        # ë©€í‹°ë¼ë²¨ threshold\n",
    "WARMUP_RATIO = 0.06    # ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ ìƒëµí•´ë„ ë¨\n",
    "PATIENCE = 2           # early stopping patience (ìœ„ì—ì„œ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ ìƒëµ)\n",
    "\n",
    "SAVE_DIR = \"/home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2.1\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "label_dict = {\n",
    "    'Communication': 0,\n",
    "    'Teamwork_Leadership': 1,\n",
    "    'Integrity': 2,\n",
    "    'Adaptability': 3,\n",
    "    'Job_Competency': 4\n",
    "}\n",
    "label_names = [k for k, _ in sorted(label_dict.items(), key=lambda x: x[1])]\n",
    "\n",
    "# =====================================\n",
    "# 1. Train / Evaluate í•¨ìˆ˜ (ë©€í‹°ë¼ë²¨ìš©)\n",
    "# =====================================\n",
    "def train_one_epoch(model, loader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train\"):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device, non_blocking=True)\n",
    "        labels         = batch[\"labels\"].to(device, non_blocking=True)  # (B, 5) float\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ëª¨ë¸: probs(sigmoid)ì™€ loss ë°˜í™˜ (BCEWithLogitsLoss)\n",
    "        probs, loss = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        all_probs.append(probs.detach().cpu().numpy())   # (B, 5)\n",
    "        all_labels.append(labels.detach().cpu().numpy()) # (B, 5)\n",
    "    \n",
    "    all_probs  = np.vstack(all_probs)   # (N, 5)\n",
    "    all_labels = np.vstack(all_labels)  # (N, 5)\n",
    "\n",
    "    # ğŸ”¹ ë©€í‹°ë¼ë²¨: thresholdë¡œ 0/1 ì˜ˆì¸¡\n",
    "    y_true = all_labels.astype(int)                      # (N, 5)\n",
    "    y_pred = (all_probs >= THRESHOLD).astype(int)        # (N, 5)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)                 # multilabel indicator ì§€ì›\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    return total_loss / len(loader), acc, macro_f1\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, target_names=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Valid\"):\n",
    "        input_ids      = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device, non_blocking=True)\n",
    "        labels         = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        probs, loss = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        if loss is not None:\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "    \n",
    "    all_probs  = np.vstack(all_probs)   # (N, 5)\n",
    "    all_labels = np.vstack(all_labels)  # (N, 5)\n",
    "\n",
    "    y_true = all_labels.astype(int)                # (N, 5)\n",
    "    y_pred = (all_probs >= THRESHOLD).astype(int)  # (N, 5)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    rep = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=target_names,\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    cm = multilabel_confusion_matrix(y_true, y_pred)  # (num_labels, 2, 2)\n",
    "\n",
    "    return total_loss / len(loader), acc, macro_f1, rep, cm\n",
    "\n",
    "# ==========================================\n",
    "# 2. Confusion Matrix ì´ë¯¸ì§€ ì €ì¥ í•¨ìˆ˜\n",
    "# ==========================================\n",
    "def save_confusion_matrix(cm, label_names, save_dir, epoch):\n",
    "    \"\"\"\n",
    "    cm : multilabel_confusion_matrix ê²°ê³¼ (shape: (num_labels, 2, 2))\n",
    "    label_names : ['Communication', ...]\n",
    "    \"\"\"\n",
    "    for i, name in enumerate(label_names):\n",
    "        matrix = cm[i]  # (2,2) = [[TN, FP], [FN, TP]]\n",
    "\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(\n",
    "            matrix,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "            yticklabels=[\"True 0\", \"True 1\"]\n",
    "        )\n",
    "        plt.title(f\"Confusion Matrix - {name} (Epoch {epoch})\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"cm_{name}_epoch{epoch}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "# ==============================\n",
    "# 3. Scheduler ìƒì„±\n",
    "#    (train_loader / optimizer\n",
    "#     ì¤€ë¹„ëœ ë’¤ì— ì‹¤í–‰í•´ì•¼ í•¨)\n",
    "# ==============================\n",
    "# âš ï¸ ì—¬ê¸°ì„œëŠ” train_loader, EPOCHS, optimizerê°€ ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\")\n",
    "\n",
    "# ==============================\n",
    "# 4. í•™ìŠµ ë£¨í”„ + Early Stopping\n",
    "#    + Best Model ì €ì¥\n",
    "#    + Confusion Matrix ì´ë¯¸ì§€ ì €ì¥\n",
    "# ==============================\n",
    "best_f1, patience = 0.0, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n==== EPOCH {epoch}/{EPOCHS} ====\")\n",
    "    \n",
    "    tr_loss, tr_acc, tr_f1 = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc, val_f1, val_rep, val_cm = evaluate(\n",
    "        model,\n",
    "        valid_loader,\n",
    "        target_names=label_names\n",
    "    )\n",
    "\n",
    "    print(f\"[Train] loss={tr_loss:.4f} acc={tr_acc:.4f} macro_f1={tr_f1:.4f}\")\n",
    "    print(f\"[Valid] loss={val_loss:.4f} acc={val_acc:.4f} macro_f1={val_f1:.4f}\")\n",
    "    print(val_rep)\n",
    "\n",
    "    # ğŸ”¥ confusion matrix ì´ë¯¸ì§€ ì €ì¥\n",
    "    save_confusion_matrix(val_cm, label_names, SAVE_DIR, epoch)\n",
    "\n",
    "    # Early Stop ê¸°ì¤€: macro F1\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1, patience = val_f1, 0\n",
    "\n",
    "        save_path = os.path.join(SAVE_DIR, \"model_best.pt\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # ë¼ë²¨ ë§µ ì €ì¥\n",
    "        with open(os.path.join(SAVE_DIR, \"label_map.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            for k, v in sorted(label_dict.items(), key=lambda x: x[1]):\n",
    "                f.write(f\"{v}\\t{k}\\n\")\n",
    "\n",
    "        print(f\">>> Best saved. macroF1={best_f1:.4f} @ {save_path}\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"patience {patience}/{PATIENCE}\")\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95281489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize_kobert_intent_v2.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quant\n",
    "from kobert_transformers import get_kobert_model\n",
    "\n",
    "# ===== 0) ì‚¬ìš©ì í™˜ê²½ =====\n",
    "MODEL_PATH = \"/home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2.1/model_best.pt\"\n",
    "SAVE_DIR = \"/home/j-k13b204/S13P31B204/model_test/kobert_intent_v5_v2/quantize\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "QUANTIZED_PT_PATH = os.path.join(SAVE_DIR, \"model_best_quantized.pt\")\n",
    "\n",
    "# ===== 1) ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (intentìš©, BCEWithLogits) =====\n",
    "class KoBertClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert,\n",
    "        hidden_size: int = 768,\n",
    "        num_classes: int = 5,\n",
    "        dr_rate: float = 0.3,\n",
    "        pos_weight: torch.Tensor | None = None,  # ê° í´ë˜ìŠ¤ì˜ ì–‘ì„± ê°€ì¤‘ì¹˜ (ë¶ˆê· í˜• ë³´ì •)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p=dr_rate) if dr_rate and dr_rate > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        if pos_weight is not None:\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        else:\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor | None = None,\n",
    "        token_type_ids: torch.Tensor | None = None,\n",
    "        labels: torch.Tensor | None = None,  # (B, num_classes), multi-hot\n",
    "    ):\n",
    "        out = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        if getattr(out, \"pooler_output\", None) is not None:\n",
    "            pooled = out.pooler_output      # (B, 768)\n",
    "        else:\n",
    "            pooled = out[0][:, 0]           # last_hidden_state[:, 0]\n",
    "\n",
    "        logits = self.classifier(self.dropout(pooled))  # (B, num_classes)\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            return probs, loss\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs, None\n",
    "\n",
    "# ===== 2) ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ =====\n",
    "def load_fp32_model(model_path: str, num_classes: int = 5, dr_rate: float = 0.3) -> nn.Module:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"[1/3] ëª¨ë¸ êµ¬ì¡° ì´ˆê¸°í™”â€¦\")\n",
    "\n",
    "    bert = get_kobert_model()  # â˜… ì–¸íŒ¨í‚¹ ì—†ì´ ëª¨ë¸ë§Œ ë°˜í™˜\n",
    "    model = KoBertClassifier(\n",
    "        bert=bert,\n",
    "        hidden_size=768,\n",
    "        num_classes=num_classes,\n",
    "        dr_rate=dr_rate,\n",
    "        pos_weight=None,   # ì¶”ë¡ ìš©ì´ë¼ pos_weight ì•ˆ ì¨ë„ ë¨\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"[2/3] ê°€ì¤‘ì¹˜ ë¡œë“œâ€¦\")\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # í•™ìŠµ ë•Œ loss_fn.* ë„ ê°™ì´ ì €ì¥ë¼ì„œ ê±¸ë¦¬ë‹ˆê¹Œ ì œê±°\n",
    "    for k in list(state.keys()):\n",
    "        if k.startswith(\"loss_fn\"):\n",
    "            del state[k]\n",
    "\n",
    "    # ì—„ê²©í•˜ê²Œ ì•ˆ ë§ì¶°ë„ ë˜ë‹ˆ strict=False\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    if missing:\n",
    "        print(f\"[WARN] missing keys: {missing}\")\n",
    "    if unexpected:\n",
    "        print(f\"[WARN] unexpected keys: {unexpected}\")\n",
    "\n",
    "    model.eval()\n",
    "    print(\"[3/3] ë¡œë“œ ì™„ë£Œ.\")\n",
    "    return model\n",
    "\n",
    "# ===== 3) ì–‘ìí™” í•¨ìˆ˜ =====\n",
    "def quantize_dynamic_int8(model: nn.Module) -> nn.Module:\n",
    "    print(\"\\n=> ë™ì  ì–‘ìí™”(INT8) ìˆ˜í–‰ ì¤‘â€¦\")\n",
    "    qmodel = quant.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8,\n",
    "    )\n",
    "    print(\"ì™„ë£Œ.\")\n",
    "    return qmodel\n",
    "\n",
    "# ===== 4) ì‹¤í–‰ë¶€ =====\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_CLASSES = 5  # NCS 5ëŒ€ ì—­ëŸ‰\n",
    "\n",
    "    model = load_fp32_model(MODEL_PATH, num_classes=NUM_CLASSES, dr_rate=0.3)\n",
    "    qmodel = quantize_dynamic_int8(model)\n",
    "\n",
    "    # ì €ì¥\n",
    "    torch.save(qmodel, QUANTIZED_PT_PATH)\n",
    "    print(f\"\\nâœ… ì–‘ìí™”ëœ intent ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {QUANTIZED_PT_PATH}\")\n",
    "\n",
    "    # í¬ê¸° ë¹„êµ\n",
    "    def size_mb(path): return os.path.getsize(path) / (1024**2)\n",
    "    orig = size_mb(MODEL_PATH)\n",
    "    quantized = size_mb(QUANTIZED_PT_PATH)\n",
    "    print(f\"\\nì›ë³¸ ëª¨ë¸ í¬ê¸°: {orig:.2f} MB\")\n",
    "    print(f\"ì–‘ìí™” ëª¨ë¸ í¬ê¸°: {quantized:.2f} MB\")\n",
    "    print(f\"ì••ì¶•ë¥ : {orig/quantized:.2f}x, í¬ê¸° ê°ì†Œ: {(orig - quantized)/orig * 100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
