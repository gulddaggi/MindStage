{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec163b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TL_01.Management_Male_Experiencedì„ ì‹œì‘í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mì„ ì¢…ë£Œ!.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (intent_empty_count, intent_count)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mintent_empty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m intent_empty(valid_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mintent_empty\u001b[39m\u001b[34m(base_path)\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     23\u001b[39m file_path = os.path.join(folder_path, file_name)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f: \n\u001b[32m     26\u001b[39m     data = json.load(f)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m qa_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m] : \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thddu\\anaconda3\\envs\\kobert_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from collections import defaultdict\n",
    "\n",
    "training_path = \"G:/ë‚´ ë“œë¼ì´ë¸Œ/ë©´ì ‘VR AIì œì‘/extracted_TrainingData\"\n",
    "valid_path = \"G:/ë‚´ ë“œë¼ì´ë¸Œ/ë©´ì ‘VR AIì œì‘/extracted_Valid\"\n",
    "\n",
    "# questionì˜ emotionì´ ë¹„ì–´ìˆê±°ë‚˜\n",
    "# answerì˜ emotionì´ ë¹„ì–´ìˆê±°ë‚˜\n",
    "def intent_empty(base_path) : \n",
    "    intent_count = 0\n",
    "    intent_empty_count = 0\n",
    "    # trainingDataì˜ í´ë”ë“¤ì„ ì½ëŠ”ë‹¤.\n",
    "    for folder_name in os.listdir(base_path) : \n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        if not os.path.isdir(folder_path) :\n",
    "            continue\n",
    "        print(f\"{folder_name}ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "        # folderë‚´ .jsoníŒŒì¼ë“¤ì„ ì½ëŠ”ë‹¤.\n",
    "        try : \n",
    "            for file_name in os.listdir(folder_path) : \n",
    "                if not file_name.endswith(\".json\") : \n",
    "                    continue\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                with open(file_path, 'r', encoding=\"utf-8\") as f: \n",
    "                    data = json.load(f)\n",
    "                \n",
    "                for qa_type in ['question','answer'] : \n",
    "                    intent = data.get('dataSet', {}).get(qa_type,{}).get('intent', [])\n",
    "                    if not intent : \n",
    "                        intent_empty_count += 1\n",
    "                    else :\n",
    "                        intent_count += 1\n",
    "\n",
    "        except Exception as e : \n",
    "            print(f\"{folder_name}ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤\")\n",
    "        print(f\"{folder_name}ì„ ì¢…ë£Œ!.\")\n",
    "    return (intent_empty_count, intent_count)\n",
    "\n",
    "intent_empty(training_path)\n",
    "intent_empty(valid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13221444",
   "metadata": {},
   "source": [
    "## intentê°€ ë¬´ì—‡ìœ¼ë¡œ ì´ë¤„ì ¸ìˆëŠ”ì§€ íŒŒì•…\n",
    "    - 52ê°œ ê°€ëŸ‰ì˜ intentê°€ ì¡´ì¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ddf2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"G:/ë‚´ ë“œë¼ì´ë¸Œ/ë©´ì ‘VR AIì œì‘/extracted_TrainingData\"\n",
    "valid_path = \"G:/ë‚´ ë“œë¼ì´ë¸Œ/ë©´ì ‘VR AIì œì‘/extracted_Valid\"\n",
    "\n",
    "\n",
    "def kinds_of_intent(base_path) : \n",
    "    intent_kinds = []\n",
    "\n",
    "    for folder_name in os.listdir(base_path) : \n",
    "        \n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        if not folder_path : \n",
    "            continue\n",
    "        try :\n",
    "            for file_name in os.listdir(folder_path) :\n",
    "                if not file_name.endswith(\".json\") : \n",
    "                    continue\n",
    "                file_path  = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                with open(file_path, 'r', encoding = \"utf-8\") as f : \n",
    "                    data = json.load(f)\n",
    "                \n",
    "                for qa_type in ['question','answer'] : \n",
    "                    intent = data.get(\"dataSet\", {}).get(qa_type, {}).get(\"intent\", [])\n",
    "\n",
    "                    for e in intent : \n",
    "                        if not isinstance(e, dict) : \n",
    "                            continue\n",
    "                        expr = e.get(\"expression\", [])\n",
    "\n",
    "                        if isinstance(expr, str) : \n",
    "                            expr = [expr]\n",
    "                        elif not isinstance(expr, list) : \n",
    "                            continue\n",
    "\n",
    "                        intent_kinds.extend(expr)\n",
    "\n",
    "        except Exception as e : \n",
    "            print(f\"{folder_name}ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤\")\n",
    "                \n",
    "    return intent_kinds\n",
    "\n",
    "def unique_intent() : \n",
    "    train_intent = kinds_of_intent(training_path)\n",
    "    valid_intent = kinds_of_intent(valid_path)\n",
    "\n",
    "    total_intent = train_intent + valid_intent\n",
    "\n",
    "    unique_intent = sorted(set(total_intent))\n",
    "\n",
    "    return unique_intent\n",
    "\n",
    "result = kinds_of_intent()\n",
    "print(result)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84436c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, csv\n",
    "\n",
    "save_dir = \"G:/ë‚´ ë“œë¼ì´ë¸Œ/ë©´ì ‘VR AIì œì‘/Code/CSVëª¨ìŒ/intent_csv\"\n",
    "\n",
    "def text_per_intent(base_path, save_name=\"text_per_intent.csv\"):\n",
    "    result = [] \n",
    "\n",
    "    for folder_name in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"{folder_name} í´ë” ì‹œì‘\")\n",
    "\n",
    "        try:\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if not file_name.endswith(\".json\"):\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                for qa_type in [\"question\", \"answer\"]:\n",
    "                    intents = data.get(\"dataSet\", {}).get(qa_type, {}).get(\"intent\", [])\n",
    "\n",
    "                    if isinstance(intents, dict):\n",
    "                        intents = [intents]\n",
    "                    if not isinstance(intents, list):\n",
    "                        continue\n",
    "\n",
    "                    for i in intents:\n",
    "                        if not isinstance(i, dict):\n",
    "                            continue\n",
    "\n",
    "                        text = i.get(\"text\", \"\").strip()\n",
    "                        if not text:\n",
    "                            continue\n",
    "\n",
    "                        expr = i.get(\"expression\", [])\n",
    "                        if isinstance(expr, str):\n",
    "                            expr = [expr]\n",
    "                        elif not isinstance(expr, list):\n",
    "                            continue\n",
    "\n",
    "                       \n",
    "                        for single_expr in expr:\n",
    "                            result.append((text, single_expr))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{folder_name} í´ë”ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    # ===== CSV ì €ì¥ =====\n",
    "    save_path = os.path.join(save_dir, save_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with open(save_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"text\", \"expression\"])\n",
    "        writer.writerows(result)\n",
    "\n",
    "    print(f\"CSV ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "\n",
    "# ===== í•¨ìˆ˜ í˜¸ì¶œ =====\n",
    "training_path = \"G:/ë‚´ ë“œë¼ì´ë¸Œ/ë©´ì ‘VR AIì œì‘/extracted_TrainingData\"\n",
    "valid_path    = \"G:/ë‚´ ë“œë¼ì´ë¸Œ/ë©´ì ‘VR AIì œì‘/extracted_Valid\"\n",
    "\n",
    "train_csv = text_per_intent(training_path, \"text_per_intent_train.csv\")\n",
    "valid_csv = text_per_intent(valid_path, \"text_per_intent_valid.csv\")\n",
    "\n",
    "print(\"Training CSV:\", train_csv)\n",
    "print(\"Valid CSV   :\", valid_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59671deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_csv = os.path.join(save_dir, \"train_text_per_intent.csv\")\n",
    "valid_csv = os.path.join(save_dir, \"valid_text_per_intent.csv\")\n",
    "total_csv = os.path.join(save_dir, \"total_intent.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "valid_df = pd.read_csv(valid_csv)\n",
    "\n",
    "total_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "\n",
    "total_df.to_csv(total_csv, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_to_group = {\n",
    "    # ğŸ—£ Communication\n",
    "    \"c_private\": \"Communication\",\n",
    "    \"c_value\": \"Communication\",\n",
    "    \"c_sincere_co\": \"Communication\",\n",
    "    \"c_person\": \"Communication\",\n",
    "    \"c_imp\": \"Communication\",\n",
    "    \"c_cop\": \"Communication\",\n",
    "    \"c_chl\": \"Communication\",\n",
    "    \"c_confl_mg\": \"Communication\",\n",
    "    \"s_pers\": \"Communication\",\n",
    "    \"s_intp\": \"Communication\",\n",
    "    \"s_comn\": \"Communication\",\n",
    "\n",
    "    # ğŸ§­ Integrity\n",
    "    \"c_sincere_job\": \"Integrity\",\n",
    "    \"p_ethic\": \"Integrity\",\n",
    "    \"p_cus_belief\": \"Integrity\",\n",
    "    \"p_voc\": \"Integrity\",\n",
    "    \"p_cus_orien\": \"Integrity\",\n",
    "    \"f_resp\": \"Integrity\",\n",
    "\n",
    "    # ğŸ”„ Adaptability\n",
    "    \"c_adp\": \"Adaptability\",\n",
    "    \"p_str_coping\": \"Adaptability\",\n",
    "    \"p_proac\": \"Adaptability\",\n",
    "    \"r_prsv\": \"Adaptability\",\n",
    "    \"r_res_mth\": \"Adaptability\",\n",
    "    \"r_res_prf\": \"Adaptability\",\n",
    "    \"i_secty\": \"Adaptability\",\n",
    "    \"i_dis_coping\": \"Adaptability\",\n",
    "    \"i_dev_mth\": \"Adaptability\",\n",
    "\n",
    "    # ğŸ¤ Teamwork / Leadership\n",
    "    \"m_sche\": \"Teamwork_Leadership\",\n",
    "    \"m_admin\": \"Teamwork_Leadership\",\n",
    "    \"m_direct\": \"Teamwork_Leadership\",\n",
    "    \"m_acct\": \"Teamwork_Leadership\",\n",
    "    \"m_holistic\": \"Teamwork_Leadership\",\n",
    "    \"m_critical\": \"Teamwork_Leadership\",\n",
    "\n",
    "    # ğŸ’¼ Job Competency\n",
    "    \"s_brand_strt\": \"Job_Competency\",\n",
    "    \"s_anls_prod\": \"Job_Competency\",\n",
    "    \"s_sell_exp\": \"Job_Competency\",\n",
    "    \"r_prof\": \"Job_Competency\",\n",
    "    \"r_info_col\": \"Job_Competency\",\n",
    "    \"r_tech_skill\": \"Job_Competency\",\n",
    "    \"i_prg\": \"Job_Competency\",\n",
    "    \"i_hwsw_prfi\": \"Job_Competency\",\n",
    "    \"i_tech_orien\": \"Job_Competency\",\n",
    "    \"d_dgn_proc\": \"Job_Competency\",\n",
    "    \"d_express\": \"Job_Competency\",\n",
    "    \"d_brand_pref\": \"Job_Competency\",\n",
    "    \"d_trendy\": \"Job_Competency\",\n",
    "    \"d_sens\": \"Job_Competency\",\n",
    "    \"d_creative\": \"Job_Competency\",\n",
    "    \"f_cons\": \"Job_Competency\",\n",
    "    \"f_prod_plan\": \"Job_Competency\",\n",
    "    \"f_equip\": \"Job_Competency\",\n",
    "    \"f_shift\": \"Job_Competency\",\n",
    "    \"f_pat\": \"Job_Competency\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5386f270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>expression</th>\n",
       "      <th>intent_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì•ì—ì„œë„ ë§ì”€ë“œë ¸ì§€ë§Œ ì €ëŠ” ì¤‘êµ­ì—ì„œ ì¼ ë…„ ì •ë„ ìœ í•™ ìƒí™œì„ í–ˆì—ˆìŠµë‹ˆë‹¤.</td>\n",
       "      <td>c_private</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì•„ ë„¤ ì €ëŠ” ì œ ë‚˜ë¦„ëŒ€ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œë²•ì€ ì–´ ì²« ë²ˆì§¸ë¡œëŠ” ë“±ì‚°ì„ ì¢‹ì•„í•©ë‹ˆë‹¤.</td>\n",
       "      <td>c_private</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì§ë¬´ ì§€ì‹ì€ ë³¸ì¸ì´ ì¡°ì§ì˜ ì¡´ì¬ê°ì„ ë‚˜íƒ€ë‚´ê³  ì—…ë¬´ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸...</td>\n",
       "      <td>c_value</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê·¸ë˜ì„œ ê²°êµ­ì€ ì§€ê¸ˆ ì „ì—­ì„ í•˜ê³  ì§€ê¸ˆ ì´ ìë¦¬ì—ì„œ ë©´ì ‘ì„ ë³´ê³  ìˆëŠ”ë° ì €ì˜ ì„ íƒì´ ...</td>\n",
       "      <td>c_sincere_co</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì–´ ì €ëŠ” ì„±ê²©ì´ ì— ì¡°ê¸ˆ ê¸‰í•œ ë¶€ë¶„ì´ ìˆì–´ìš”.</td>\n",
       "      <td>c_person</td>\n",
       "      <td>Communication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    expression  \\\n",
       "0           ì•ì—ì„œë„ ë§ì”€ë“œë ¸ì§€ë§Œ ì €ëŠ” ì¤‘êµ­ì—ì„œ ì¼ ë…„ ì •ë„ ìœ í•™ ìƒí™œì„ í–ˆì—ˆìŠµë‹ˆë‹¤.     c_private   \n",
       "1        ì•„ ë„¤ ì €ëŠ” ì œ ë‚˜ë¦„ëŒ€ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œë²•ì€ ì–´ ì²« ë²ˆì§¸ë¡œëŠ” ë“±ì‚°ì„ ì¢‹ì•„í•©ë‹ˆë‹¤.     c_private   \n",
       "2  ì§ë¬´ ì§€ì‹ì€ ë³¸ì¸ì´ ì¡°ì§ì˜ ì¡´ì¬ê°ì„ ë‚˜íƒ€ë‚´ê³  ì—…ë¬´ ìˆ˜í–‰í•˜ëŠ” ë° ìˆì–´ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸...       c_value   \n",
       "3  ê·¸ë˜ì„œ ê²°êµ­ì€ ì§€ê¸ˆ ì „ì—­ì„ í•˜ê³  ì§€ê¸ˆ ì´ ìë¦¬ì—ì„œ ë©´ì ‘ì„ ë³´ê³  ìˆëŠ”ë° ì €ì˜ ì„ íƒì´ ...  c_sincere_co   \n",
       "4                          ì–´ ì €ëŠ” ì„±ê²©ì´ ì— ì¡°ê¸ˆ ê¸‰í•œ ë¶€ë¶„ì´ ìˆì–´ìš”.      c_person   \n",
       "\n",
       "    intent_group  \n",
       "0  Communication  \n",
       "1  Communication  \n",
       "2  Communication  \n",
       "3  Communication  \n",
       "4  Communication  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./CSVëª¨ìŒ/intent_csv/intent_grouping.csv\", index_col = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"intent_group\"] = data[\"expression\"].map(expression_to_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba455739",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"/content/drive/MyDrive/ë©´ì ‘VR AIì œì‘/Code/CSVëª¨ìŒ/intent_csv/intent_grouping.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d56eef18-b124-4964-8e5d-90ecd5caa90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intent_group\n",
       "Communication          19963\n",
       "Integrity               4479\n",
       "Job_Competency          3777\n",
       "Adaptability            2549\n",
       "Teamwork_Leadership     1706\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['intent_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a849e0-f865-4e11-b980-10c260aea5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q kobert-transformers==0.5.1 transformers==4.30.2 sentencepiece scikit-learn matplotlib\n",
    "\n",
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from kobert_transformers import get_kobert_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# ê²½ë¡œ & í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# =========================\n",
    "CSV_PATH  = \"./CSVëª¨ìŒ/intent_csv/intent_grouping.csv\"\n",
    "SAVE_DIR  = \"./model\"\n",
    "\n",
    "NUM_LABELS     = 5   # 0:Comm, 1:Int, 2:Adapt, 3:Team, 4:Job\n",
    "MAX_LEN        = 128\n",
    "BATCH_SIZE     = 32\n",
    "EPOCHS         = 10\n",
    "LR             = 3e-5\n",
    "WARMUP_RATIO   = 0.06\n",
    "WEIGHT_DECAY   = 0.01\n",
    "MAX_GRAD_NORM  = 1.0\n",
    "DROPOUT        = 0.3\n",
    "PATIENCE       = 3  # ì‚´ì§ ëŠ˜ë¦¼\n",
    "USE_SAMPLER    = True  # ê°€ì¤‘ ìƒ˜í”ŒëŸ¬ ì‚¬ìš© ì—¬ë¶€\n",
    "\n",
    "SEED   = 42\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "id2label = {\n",
    "    0:\"Communication\",\n",
    "    1:\"Integrity\",\n",
    "    2:\"Adaptability\",\n",
    "    3:\"Teamwork_Leadership\",\n",
    "    4:\"Job_Competency\"\n",
    "}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "# =========================\n",
    "# ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬ (ë©€í‹°ë¼ë²¨)\n",
    "# =========================\n",
    "TEXT_COL  = \"text\"\n",
    "LABEL_COL = \"intent_group_id\"  # \"[0, 3]\" ë˜ëŠ” ë‹¨ì¼ê°’\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str).str.strip()\n",
    "df = df[df[TEXT_COL] != \"\"]\n",
    "\n",
    "def parse_labels(x):\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            parts = s[1:-1].split(\",\")\n",
    "            return [int(p.strip()) for p in parts if p.strip().isdigit()]\n",
    "        elif s.isdigit():\n",
    "            return [int(s)]\n",
    "        else:\n",
    "            parts = [t for t in s.replace(\",\", \" \").split() if t.isdigit()]\n",
    "            return [int(p) for p in parts]\n",
    "    elif isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return [int(v) for v in x]\n",
    "    else:\n",
    "        return [int(x)]\n",
    "\n",
    "def to_multihot(label_list, num_labels):\n",
    "    vec = np.zeros(num_labels, dtype=np.float32)\n",
    "    for l in label_list:\n",
    "        if 0 <= l < num_labels:\n",
    "            vec[l] = 1.0\n",
    "    return vec\n",
    "\n",
    "df[\"labels\"]    = df[LABEL_COL].apply(parse_labels)\n",
    "df[\"label_vec\"] = df[\"labels\"].apply(lambda x: to_multihot(x, NUM_LABELS))\n",
    "\n",
    "# =========================\n",
    "# ëˆ„ìˆ˜ ë°©ì§€ ë¶„í•  (ë™ì¼ í…ìŠ¤íŠ¸ ë‹¨ìœ„ ê·¸ë£¹)\n",
    "# =========================\n",
    "groups = df[TEXT_COL].astype(str)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=groups))\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"train: {len(train_df)}, valid: {len(val_df)}\")\n",
    "\n",
    "# =========================\n",
    "# Tokenizer & BERT ë¡œë“œ\n",
    "# =========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
    "bert = get_kobert_model()  # BaseModelOutputWithPooling ë°˜í™˜\n",
    "\n",
    "# =========================\n",
    "# Dataset / DataLoader\n",
    "# =========================\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df[TEXT_COL].tolist()\n",
    "        self.labels = np.stack(df[\"label_vec\"].to_numpy())\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(\n",
    "            self.texts[i],\n",
    "            truncation=True, padding=\"max_length\", max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[i], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "train_dataset = MultiLabelDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_dataset   = MultiLabelDataset(val_df, tokenizer, MAX_LEN)\n",
    "\n",
    "# ----- (ì˜µì…˜) ê°€ì¤‘ ìƒ˜í”ŒëŸ¬ -----\n",
    "sampler = None\n",
    "if USE_SAMPLER:\n",
    "    mat = np.stack(train_df[\"label_vec\"].to_numpy())   # (N,C)\n",
    "    pos = mat.sum(axis=0)                              # (C,)\n",
    "    neg = mat.shape[0] - pos\n",
    "    eps = 1e-6\n",
    "    cls_weight = (neg + eps) / (pos + eps)             # í¬ì†Œ ë¼ë²¨ì¼ìˆ˜ë¡ í¼\n",
    "    sample_weights = (mat * cls_weight).sum(axis=1)    # ê° ìƒ˜í”Œì˜ ë¼ë²¨ ê°€ì¤‘ì¹˜ í•©\n",
    "    sample_weights = np.where(sample_weights > 0, sample_weights, 1.0)\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.tensor(sample_weights, dtype=torch.double),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler if USE_SAMPLER else None,\n",
    "    shuffle=not USE_SAMPLER,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# =========================\n",
    "# ëª¨ë¸ ì •ì˜ (ë‹¤ì¤‘ ë¼ë²¨)\n",
    "# =========================\n",
    "class KoBERTMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, bert, num_labels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(768, num_labels)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output if hasattr(outputs, \"pooler_output\") else outputs[1]\n",
    "        logits = self.fc(self.drop(pooled))  # (B, C)\n",
    "        return logits\n",
    "\n",
    "model = KoBERTMultiLabelClassifier(bert, num_labels=NUM_LABELS, dropout=DROPOUT).to(DEVICE)\n",
    "\n",
    "# ----- ë¡œì§“ ë°”ì´ì–´ìŠ¤ ì´ˆê¸°í™”(ì„ í—˜ í™•ë¥ ) -----\n",
    "with torch.no_grad():\n",
    "    label_mat = np.stack(train_df[\"label_vec\"].to_numpy())  # (N,C)\n",
    "    pos = label_mat.sum(axis=0)\n",
    "    neg = label_mat.shape[0] - pos\n",
    "    p = np.clip(pos / (pos + neg + 1e-6), 1e-4, 1-1e-4)\n",
    "    init_bias = torch.tensor(np.log(p / (1-p)), dtype=torch.float32).to(DEVICE)\n",
    "    model.fc.bias.copy_(init_bias)\n",
    "\n",
    "# =========================\n",
    "# ì†ì‹¤/ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¤„ëŸ¬/AMP\n",
    "# =========================\n",
    "# ---- pos_weight: ë¶ˆê· í˜• ë³´ì • ----\n",
    "pos_weight_vec = torch.tensor((neg + 1e-6) / (pos + 1e-6), dtype=torch.float32).to(DEVICE)\n",
    "print(\"pos_weight:\", pos_weight_vec.detach().cpu().numpy())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_vec)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "# =========================\n",
    "# í† í¬ë‚˜ì´ì € ì•ˆì „ ì €ì¥ (ë ˆê±°ì‹œ í˜¸í™˜)\n",
    "# =========================\n",
    "def safe_save_tokenizer(tok, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    try:\n",
    "        tok.save_pretrained(save_dir)\n",
    "        return\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        if hasattr(tok, \"save_vocabulary\"):\n",
    "            tok.save_vocabulary(save_dir)\n",
    "        try:\n",
    "            with open(os.path.join(save_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(getattr(tok, \"special_tokens_map\", {}), f, ensure_ascii=False, indent=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(getattr(tok, \"init_kwargs\", {}), f, ensure_ascii=False, indent=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"Tokenizer saved with legacy-compatible routine.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenizer save skipped due to error: {e}\")\n",
    "\n",
    "# =========================\n",
    "# ë¼ë²¨ë³„ ì„ê³„ê°’ íƒìƒ‰\n",
    "# =========================\n",
    "def find_optimal_thresholds(model, loader, device, num_labels, grid=None):\n",
    "    model.eval()\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    all_probs, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            att = batch[\"attention_mask\"].to(device)\n",
    "            y   = batch[\"labels\"].cpu().numpy()\n",
    "            logits = model(ids, att)\n",
    "            probs  = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.append(probs); all_true.append(y)\n",
    "    P = np.vstack(all_probs)\n",
    "    Y = np.vstack(all_true)\n",
    "\n",
    "    best = np.full(num_labels, 0.5, dtype=np.float32)\n",
    "    for c in range(num_labels):\n",
    "        best_f1, best_t = -1.0, 0.5\n",
    "        for t in grid:\n",
    "            pred_c = (P[:, c] >= t).astype(int)\n",
    "            true_c = Y[:, c].astype(int)\n",
    "            f1 = f1_score(true_c, pred_c, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "        best[c] = best_t\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# í‰ê°€ í•¨ìˆ˜ (ë¼ë²¨ë³„ ì„ê³„ê°’ ì ìš©)\n",
    "# =========================\n",
    "THRESH_VEC = torch.full((NUM_LABELS,), 0.5, dtype=torch.float32)  # ì´ˆê¸°ê°’\n",
    "\n",
    "def evaluate(return_preds=False):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            att = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            with torch.amp.autocast('cuda', enabled=(DEVICE.type == \"cuda\")):\n",
    "                logits = model(ids, att)\n",
    "                probs  = torch.sigmoid(logits)\n",
    "            preds = (probs.cpu() >= THRESH_VEC).float()\n",
    "            all_preds.append(preds.numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    y_pred = np.vstack(all_preds); y_true = np.vstack(all_labels)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    exact_match_acc = (y_pred == y_true).mean()\n",
    "    if return_preds:\n",
    "        return exact_match_acc, f1_macro, f1_micro, y_true, y_pred\n",
    "    return exact_match_acc, f1_macro, f1_micro\n",
    "\n",
    "# =========================\n",
    "# í•™ìŠµ ë£¨í”„ + ì¡°ê¸° ì¢…ë£Œ\n",
    "# =========================\n",
    "best_f1, wait = -1.0, 0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Train]\")\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE.type == \"cuda\")):\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # ---- ì—í­ ë§ì— ì„ê³„ê°’ íŠœë‹(ê²€ì¦ì…‹) -> í‰ê°€ ----\n",
    "    best_thresh = find_optimal_thresholds(model, val_loader, DEVICE, NUM_LABELS)\n",
    "    THRESH_VEC[:] = torch.tensor(best_thresh, dtype=torch.float32)\n",
    "    acc, f1_macro, f1_micro = evaluate()\n",
    "    print(f\"Epoch {epoch} | train_loss {total_loss/len(train_loader):.4f} \"\n",
    "          f\"| val_acc {acc:.4f} | val_macroF1 {f1_macro:.4f} | val_microF1 {f1_micro:.4f} \"\n",
    "          f\"| thresholds {best_thresh}\")\n",
    "\n",
    "    if f1_macro > best_f1:\n",
    "        best_f1 = f1_macro\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"model.pt\"))\n",
    "        safe_save_tokenizer(tokenizer, SAVE_DIR)\n",
    "        with open(os.path.join(SAVE_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"num_labels\": NUM_LABELS,\n",
    "                \"max_len\": MAX_LEN,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"lr\": LR,\n",
    "                \"warmup_ratio\": WARMUP_RATIO,\n",
    "                \"weight_decay\": WEIGHT_DECAY,\n",
    "                \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "                \"dropout\": DROPOUT,\n",
    "                \"thresholds\": best_thresh.tolist(),\n",
    "                \"use_sampler\": USE_SAMPLER\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        with open(os.path.join(SAVE_DIR, \"label_map.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            for k, v in id2label.items():\n",
    "                f.write(f\"{k}:{v}/n\")\n",
    "        print(f\"  â†’ best model saved (macroF1={best_f1:.4f}) to {SAVE_DIR}\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait > PATIENCE:\n",
    "            print(\"Early stopping.\"); break\n",
    "\n",
    "print(\"Training done.\")\n",
    "\n",
    "# =========================\n",
    "# Best ë¡œë“œ í›„ ìµœì¢… í‰ê°€/ë¦¬í¬íŠ¸ ì €ì¥\n",
    "# =========================\n",
    "best_model = KoBERTMultiLabelClassifier(get_kobert_model(), num_labels=NUM_LABELS, dropout=DROPOUT).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"model.pt\"), map_location=DEVICE))\n",
    "best_model.eval()\n",
    "\n",
    "# ì €ì¥ëœ thresholds ë¶ˆëŸ¬ì™€ ì ìš©\n",
    "try:\n",
    "    with open(os.path.join(SAVE_DIR, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = json.load(f)\n",
    "    if \"thresholds\" in cfg:\n",
    "        THRESH_VEC[:] = torch.tensor(cfg[\"thresholds\"], dtype=torch.float32)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "acc, f1_macro, f1_micro, y_true, y_pred = evaluate(return_preds=True)\n",
    "print(f\"[BEST] exact_match_acc={acc:.4f} | macroF1={f1_macro:.4f} | microF1={f1_micro:.4f}\")\n",
    "\n",
    "report = classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=[f\"{i}:{id2label[i]}\" for i in range(NUM_LABELS)],\n",
    "    zero_division=0\n",
    ")\n",
    "with open(os.path.join(SAVE_DIR, \"classification_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report)\n",
    "print(\"Saved classification_report.txt\")\n",
    "\n",
    "mcm = multilabel_confusion_matrix(y_true, y_pred, labels=list(range(NUM_LABELS)))\n",
    "for i, cm in enumerate(mcm):\n",
    "    print(f\"/nLabel {i} - {id2label[i]} confusion matrix:/n{cm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
