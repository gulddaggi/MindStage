import torch
import torch.nn as nn
from kobert_transformers import get_tokenizer
import re
import logging

# Suppress verbose HuggingFace HTTP debug logs
logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.WARNING)
logging.getLogger("huggingface_hub").setLevel(logging.WARNING)

# âš ï¸ ì–‘ìí™” ëª¨ë¸ì€ CPU ì „ìš©!
device = torch.device("cpu")


class BertClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size: int = 768,
                 num_classes: int = 5,
                 dr_rate: float = 0.3,
                 class_weights: torch.Tensor | None = None):
        super().__init__()
        self.bert = bert
        self.dropout = nn.Dropout(p=dr_rate) if dr_rate and dr_rate > 0 else nn.Identity()
        self.classifier = nn.Linear(hidden_size, num_classes)

        # ğŸ” BCEWithLogitsLoss ì‚¬ìš© (Sigmoid ê¸°ë°˜ ë©€í‹°ë¼ë²¨ í•™ìŠµìš©)
        # class_weightsê°€ ìˆì„ ê²½ìš° BCEWithLogitsLossì˜ pos_weightë¡œ ì‚¬ìš©
        if class_weights is not None:
            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)
        else:
            self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self,
                input_ids: torch.Tensor,
                attention_mask: torch.Tensor | None = None,
                token_type_ids: torch.Tensor | None = None,
                labels: torch.Tensor | None = None):

        outputs = self.bert(input_ids=input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids)

        if hasattr(outputs, "pooler_output") and outputs.pooler_output is not None:
            pooled = outputs.pooler_output
        else:
            # ë§ˆì§€ë§‰ hidden stateì˜ [CLS] í† í° ì‚¬ìš©
            pooled = outputs[0][:, 0]

        logits = self.classifier(self.dropout(pooled))

        if labels is not None:
            # BCEWithLogitsLossëŠ” float íƒ€ê²Ÿ ê¸°ëŒ€
            loss = self.loss_fn(logits, labels.float())
            return logits, loss
        return logits, None


def load_quantized_model(model_path, label_map_path):
    """ì–‘ìí™”ëœ ëª¨ë¸ê³¼ ë¼ë²¨ ë§¤í•‘ ë¡œë“œ (CPU ì „ìš©)"""

    print("âš ï¸  ì–‘ìí™” ëª¨ë¸ì€ CPUì—ì„œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    print("ğŸ’¡ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì›ë³¸ ëª¨ë¸(model.pt)ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n")

    # ë¼ë²¨ ë§¤í•‘ ë¡œë“œ (idx \t label)
    label_map = {}
    with open(label_map_path, 'r', encoding='utf-8') as f:
        for line in f:
            idx, label = line.strip().split('\t')
            label_map[int(idx)] = label

    print("ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì¤‘...")

    import sys as _sys
    main_mod = _sys.modules.get('__main__')
    if main_mod is not None:

        for name in ("KoBertClassifier", "BertClassifier"):
            if not hasattr(main_mod, name):
                setattr(main_mod, name, BertClassifier)

    # FutureWarningì€ ìš°ë¦¬ê°€ ì‹ ë¢°í•˜ëŠ” ìê¸° íŒŒì¼ì´ë‹ˆê¹Œ ê·¸ëƒ¥ weights_only=False ëª…ì‹œ
    try:
        model = torch.load(model_path, map_location='cpu', weights_only=False)
    except TypeError:
        # Torch ë²„ì „ì— ë”°ë¼ weights_only ì¸ìê°€ ì—†ì„ ìˆ˜ë„ ìˆì–´ì„œ fallback
        model = torch.load(model_path, map_location='cpu')

    model.eval()
    print("âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (CPU ëª¨ë“œ)\n")
    return model, label_map


def split_sentences(text: str):
    """ë¬¸ë‹¨ì„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬"""
    # ì¤„ë°”ê¿ˆ/ì—¬ëŸ¬ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text.strip())

    # ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ê¸°ì¤€ + ê³µë°±)
    sentences = re.split(r'(?<=[.!?])\s+', text)

    # ë¹ˆ ë¬¸ì¥ ì œê±°
    sentences = [s.strip() for s in sentences if s.strip()]

    return sentences


@torch.no_grad()
def predict_intent(model, tokenizer, sentence, label_map, max_len: int = 502):
    """
    ë‹¨ì¼ ë¬¸ì¥ì˜ ì˜ë„ ì˜ˆì¸¡
    - Sigmoid + BCEWithLogitsLossë¡œ í•™ìŠµëœ ë©€í‹°ë¼ë²¨ ëª¨ë¸ì„
      "Top-1 + Top-2 ì—­ëŸ‰" í˜•íƒœë¡œ í•´ì„
    """
    model.eval()

    # í† í¬ë‚˜ì´ì§•
    encoding = tokenizer(
        sentence,
        padding='max_length',
        truncation=True,
        max_length=max_len,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']
    token_type_ids = encoding.get('token_type_ids')

    # ì˜ˆì¸¡
    logits, _ = model(input_ids, attention_mask, token_type_ids)

    # ğŸ” BCEWithLogitsLoss ê¸°ë°˜ì´ë¯€ë¡œ softmaxê°€ ì•„ë‹ˆë¼ sigmoid ì‚¬ìš©
    # logits: [1, num_classes] â†’ probs: [num_classes]
    probs = torch.sigmoid(logits)[0]  # shape: (num_classes,)

    # ğŸ” Top-2 ì¸ë±ìŠ¤ì™€ ê°’
    topk_vals, topk_idx = torch.topk(probs, k=2)
    topk_vals = topk_vals.tolist()
    topk_idx = topk_idx.tolist()

    # 1ë“± ì •ë³´ (ê¸°ì¡´ê³¼ ë™ì¼)
    pred_idx = topk_idx[0]
    confidence = topk_vals[0]

    # Top-2ë¥¼ (label, prob) ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±
    top2 = [(label_map[i], v) for i, v in zip(topk_idx, topk_vals)]

    return label_map[pred_idx], confidence, probs.numpy(), top2


def analyze_paragraph(model, tokenizer, paragraph: str, label_map):
    """ë¬¸ë‹¨ ì „ì²´ ë¶„ì„"""
    sentences = split_sentences(paragraph)

    # print("=" * 80)
    # print("ğŸ“ ë¬¸ë‹¨ ë¶„ì„ ê²°ê³¼")
    # print("=" * 80)
    # print(f"\nì´ {len(sentences)}ê°œì˜ ë¬¸ì¥ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n")

    results = []

    for i, sentence in enumerate(sentences, 1):
        intent, confidence, probs, top2 = predict_intent(model, tokenizer, sentence, label_map)
        results.append({
            'sentence_num': i,
            'sentence': sentence,
            'intent': intent,
            'confidence': confidence,
            'probabilities': probs,
            'top2': top2,
        })

        # print(f"[ë¬¸ì¥ {i}]")
        # print(f"ë‚´ìš©: {sentence}")
        # print(f"ë¶„ë¥˜(Top-1): {intent} (í™•ì‹ ë„: {confidence:.2%})")

        # # ğŸ” Top-2 ì—­ëŸ‰ ìš”ì•½ ì¶œë ¥
        # print("Top-2 ì—­ëŸ‰:")
        # for rank, (lbl, val) in enumerate(top2, 1):
        #     print(f"  {rank}) {lbl}: {val:.2%}")

        # # ì „ì²´ ì„¸ë¶€ í™•ë¥  ì¶œë ¥
        # print("ì„¸ë¶€ í™•ë¥ :")
        # for idx, prob in enumerate(probs):
        #     print(f"  - {label_map[idx]}: {prob:.2%}")
        # print("-" * 80)

    return results


def print_summary(results):
    """ìš”ì•½ í†µê³„ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ë¶„ì„ ìš”ì•½")
    print("=" * 80)

    # Top-1 ê¸°ì¤€ ë¬¸ì¥ ìˆ˜ ì§‘ê³„
    intent_counts = {}
    for result in results:
        intent = result['intent']
        intent_counts[intent] = intent_counts.get(intent, 0) + 1

    print("\nì˜ë„ë³„ ë¬¸ì¥ ìˆ˜ (Top-1 ê¸°ì¤€):")
    for intent, count in sorted(intent_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(results)) * 100
        print(f"  {intent}: {count}ê°œ ({percentage:.1f}%)")

    avg_confidence = sum(r['confidence'] for r in results) / len(results)
    print(f"\ní‰ê·  í™•ì‹ ë„(Top-1): {avg_confidence:.2%}")


# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    print("=" * 80)
    print("ğŸš€ ì–‘ìí™” KoBERT ëª¨ë¸ ì¶”ë¡  (CPU ìµœì í™”, Sigmoid + BCEWithLogitsLoss)")
    print("=" * 80)
    print()

    # ğŸ” Windows ê²½ë¡œëŠ” raw string(r"...") ì‚¬ìš© ê¶Œì¥
    MODEL_PATH = "ai/v1_code/using_custom_models/model_intent_v2_quantized.pt"
    LABEL_MAP_PATH = "ai/v1_code/using_custom_models/label_map.txt"

    print("ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
    tokenizer = get_tokenizer()
    model, label_map = load_quantized_model(MODEL_PATH, LABEL_MAP_PATH)

    # ë¶„ì„í•  ë¬¸ë‹¨
    sentence = """
ê°€ì¥ ëª°ì…í–ˆë˜ ê²½í—˜ì€ ì‹œê°ì¥ì• ì¸ì„ ìœ„í•œ ììœ¨ì£¼í–‰ RCì¹´ ê°œë°œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆë˜ ê²ƒì…ë‹ˆë‹¤.
ë‹¹ì‹œ WebRTC ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ë³´í˜¸ì ëŒ€ì‹œë³´ë“œì— RCì¹´ê°€ ì˜ìƒì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì†¡ì‹ í•˜ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ì–‘ì˜ í•œê³„ë¡œ ì¸í•´, ì¹´ë©”ë¼ ì˜ìƒì„ ì§ì ‘ì ìœ¼ë¡œ ë°›ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ì´ì— ì ‘ê·¼ ë°©ì‹ì„ ì˜ìƒ ì „ì†¡ì—ì„œ ì´ë¯¸ì§€ í”„ë ˆì„ ì „ì†¡ìœ¼ë¡œ ë³€ê²½í•˜ì˜€ê³ , ì¢Œí‘œ ë©”íƒ€ë°ì´í„°ë¡œ AI ê°ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ ì „ì†¡í•˜ë ¤ë˜ ë¶€ë¶„ì„ ì´ë¯¸ì§€ í”„ë ˆì„ í•˜ë‚˜ë¡œ í•¨ê»˜ ì „ì†¡í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ë³´í˜¸ì ëŒ€ì‹œë³´ë“œì—ì„œëŠ” ê°ì²´ íƒì§€ ë°•ìŠ¤ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ì—°ì†ì ìœ¼ë¡œ í‘œì‹œí•˜ì—¬ ì˜ìƒì²˜ëŸ¼ ë³´ì´ë„ë¡ ë§Œë“¤ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
ë™ì‹œì— AI ê°ì²´ íƒì§€ë¥¼ ìœ„í•´ì„œ ë°ì´í„°ë¥¼ í•™ìŠµí•´ì•¼ í–ˆëŠ”ë° ì‹¤ë‚´ ì´ìš©ì´ ê°€ëŠ¥í•¨ì„ MVPë¡œ ë§Œë“¤ê³ ì í•˜ì˜€ê¸°ì— ì˜ìë‚˜ ì±…ìƒ, ê°€ë°©, ì‚¬ëŒ, ë²½ê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ì£¼ë¡œ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê°™ì€ ì˜ìë¼ê³  í•´ë„ ë³´ì—¬ì§€ëŠ” ì‹œì ì— ë”°ë¼ ì´ë¯¸ì§€ê°€ ì™„ì „íˆ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ë§ì€ ë°ì´í„°ê°€ ì˜¤íˆë ¤ ì •í™•ë„ë¥¼ ë–¨ì–´ëœ¨ë¦¬ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤. ì´ì— ë°ì´í„°ë¥¼ ë‹¤ì‹œê¸ˆ ì •ë¹„í•˜ê³ , ì§ì ‘ ì‚¬ì§„ì„ ì°ì–´ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ë“±ì˜ ì ‘ê·¼ì„ ì·¨í•˜ì ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
ì´ëŸ¬í•œ ê²½í—˜ë“¤ì„ í†µí•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ìš© ì‹œì—ëŠ” ì‚¬ì „ì— ì‚¬ì–‘ì„ í™•ì‹¤í•˜ê²Œ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ê³ , AIì—ì„œëŠ” ë§ì€ ë°ì´í„°ê°€ í•­ìƒ ì¢‹ì§€ëŠ” ì•Šë‹¤ëŠ” ì ì„ ë°°ì› ìœ¼ë©° ì´ëŠ” ì´í›„ ì—…ë¬´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.
"""

    # ë¶„ì„ ì‹¤í–‰
    results = analyze_paragraph(model, tokenizer, sentence, label_map)

    # ìš”ì•½ ì¶œë ¥
    print_summary(results)

    print("\n" + "=" * 80)
    print("ğŸ’¡ íŒ:")
    print("  - ì–‘ìí™” ëª¨ë¸ì€ ì›ë³¸ ëŒ€ë¹„ ìš©ëŸ‰ì´ ì‘ê³  ì¶”ë¡  ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤")
    print("  - CPUì—ì„œ ìµœì í™”ë˜ì–´ ìˆì–´ GPU ì—†ì´ë„ ë¹ ë¥¸ ì¶”ë¡  ê°€ëŠ¥")
    print("  - FastAPI ë“± í”„ë¡œë•ì…˜ ë°°í¬ì— ì í•©í•©ë‹ˆë‹¤")
    print("=" * 80)
