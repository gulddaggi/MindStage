# kobert_full_test.py
import torch
from kobert_transformers import get_kobert_model, get_tokenizer

def main():
    # 1ï¸âƒ£ Load tokenizer and model
    print("Loading KoBERT tokenizer and model...")
    tokenizer = get_tokenizer()
    model = get_kobert_model()

    # 2ï¸âƒ£ Input sentence
    sentence = "ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ BERT ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."

    # 3ï¸âƒ£ Tokenization (text â†’ subwords)
    tokens = tokenizer.tokenize(sentence)
    print("\nğŸ§© Tokenized subwords:")
    print(tokens)

    # 4ï¸âƒ£ Convert to IDs and prepare model input
    inputs = tokenizer(
        sentence,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=64
    )
    print("\nğŸ“˜ Token IDs:", inputs["input_ids"])
    print("ğŸ“— Attention mask:", inputs["attention_mask"])

    # 5ï¸âƒ£ Run the model forward pass
    with torch.no_grad():
        outputs = model(**inputs)

    # 6ï¸âƒ£ Inspect the result
    print("\nâœ… Model loaded successfully and ran inference!")
    print("Output shape:", outputs.last_hidden_state.shape)
    print("First token vector (first 5 dims):")
    print(outputs.last_hidden_state[0, 0, :5])

    # 7ï¸âƒ£ Decode token IDs back to text
    decoded = tokenizer.decode(inputs["input_ids"][0])
    print("\nğŸ” Decoded back to text:")
    print(decoded)

if __name__ == "__main__":
    main()
